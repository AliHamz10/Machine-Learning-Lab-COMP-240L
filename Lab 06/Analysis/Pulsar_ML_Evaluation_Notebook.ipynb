{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6843d2f",
   "metadata": {},
   "source": [
    "\n",
    "# ML Evaluation Basics on Pulsar Stars (Kaggle)  \n",
    "**Metrics & Tools:** K‑fold Validation, Confusion Matrix, Accuracy, Precision, Recall, F1‑score, Over/Underfitting check (Learning Curve), ROC Curve, *p‑value* and *t‑test*.\n",
    "\n",
    "**Dataset:** `pulsar_stars.csv` (HTRU2) — upload next to this notebook or to `/mnt/data/pulsar_stars.csv` and re‑run.  \n",
    "⚠️ 'pulsar_stars.csv' not found. Using a synthetic fallback dataset (shape and behavior roughly resemble the real one). Upload the CSV and re-run to use the real data.\n",
    "\n",
    "This notebook demonstrates an end‑to‑end ML workflow with clear code comments and plots, suitable for beginners.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, learning_curve, cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, RocCurveDisplay, classification_report\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Matplotlib only; one chart per figure; no custom colors (per constraints).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to load the Kaggle CSV; otherwise make a synthetic fallback so the notebook still runs.\n",
    "def make_synthetic_pulsar_like(n=3000, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    n1 = int(0.15 * n)\n",
    "    n0 = n - n1\n",
    "    X0 = rng.normal(loc=0.0, scale=1.0, size=(n0, 8))\n",
    "    X1 = rng.normal(loc=0.5, scale=1.0, size=(n1, 8))\n",
    "    X = np.vstack([X0, X1])\n",
    "    y = np.hstack([np.zeros(n0, dtype=int), np.ones(n1, dtype=int)])\n",
    "    cols = [\n",
    "        \"Mean of the integrated profile\",\n",
    "        \"Standard deviation of the integrated profile\",\n",
    "        \"Excess kurtosis of the integrated profile\",\n",
    "        \"Skewness of the integrated profile\",\n",
    "        \"Mean of the DM-SNR curve\",\n",
    "        \"Standard deviation of the DM-SNR curve\",\n",
    "        \"Excess kurtosis of the DM-SNR curve\",\n",
    "        \"Skewness of the DM-SNR curve\",\n",
    "    ]\n",
    "    df = pd.DataFrame(X, columns=cols)\n",
    "    df[\"target_class\"] = y\n",
    "    return df\n",
    "\n",
    "possible_paths = [\"/mnt/data/pulsar_stars.csv\", \"pulsar_stars.csv\"]\n",
    "csv_path = next((p for p in possible_paths if os.path.exists(p)), None)\n",
    "\n",
    "if csv_path:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"Loaded dataset from: {csv_path}\")\n",
    "else:\n",
    "    df = make_synthetic_pulsar_like()\n",
    "    print(\"⚠️ 'pulsar_stars.csv' not found. Using synthetic fallback. Upload the real CSV and re‑run.\")\n",
    "\n",
    "target_col_candidates = [c for c in df.columns if c.lower() in [\"target\", \"target_class\", \"class\", \"label\"]]\n",
    "target_col = target_col_candidates[0] if target_col_candidates else df.columns[-1]\n",
    "feature_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "for c in feature_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "df = df.dropna()\n",
    "\n",
    "X = df[feature_cols].values\n",
    "y = df[target_col].values.astype(int)\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Target counts:\\n\", pd.Series(y).value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee16af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train/test split (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Pipeline: Standardize -> Logistic Regression\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(max_iter=200, solver=\"lbfgs\"))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092be249",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# K-fold cross-validation (5-fold) with multiple metrics\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]\n",
    "cv_results = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False)\n",
    "\n",
    "for metric in scoring:\n",
    "    vals = cv_results[f\"test_{metric}\"]\n",
    "    print(f\"{metric:>9}: mean={vals.mean():.4f}, std={vals.std():.4f}, folds={vals.round(4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02530065",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit on the full training set and evaluate on the hold‑out test set\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "y_prob = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nTEST metrics:\")\n",
    "print(\"Accuracy :\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred, zero_division=0), 4))\n",
    "print(\"Recall   :\", round(recall_score(y_test, y_pred, zero_division=0), 4))\n",
    "print(\"F1 Score :\", round(f1_score(y_test, y_pred, zero_division=0), 4))\n",
    "print(\"ROC AUC  :\", round(roc_auc_score(y_test, y_prob), 4))\n",
    "\n",
    "print(\"\\nClassification report (TEST):\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e728bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion Matrix (Test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot(ax=ax)\n",
    "ax.set_title(\"Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC Curve (Test)\n",
    "fpr, tpr, thresh = roc_curve(y_test, y_prob)\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax)\n",
    "ax.set_title(\"ROC Curve\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad04371",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learning Curve to check over/underfitting\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    pipe, X_train, y_train, cv=cv, scoring=\"accuracy\",\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5), shuffle=True, random_state=42\n",
    ")\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(train_sizes, train_mean, marker=\"o\", label=\"Train score\")\n",
    "ax.plot(train_sizes, val_mean, marker=\"s\", label=\"Validation score\")\n",
    "ax.set_xlabel(\"Training set size\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Learning Curve (Over/Underfitting Check)\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "gap = train_mean[-1] - val_mean[-1]\n",
    "if val_mean[-1] < 0.7:\n",
    "    print(\"Model may be underfitting (validation accuracy is relatively low).\")\n",
    "elif gap > 0.05:\n",
    "    print(\"Potential overfitting (noticeable train > validation gap).\")\n",
    "else:\n",
    "    print(\"No strong signs of over/underfitting; curves are fairly close.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81268c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# p‑value via independent t‑test on the first feature (class 0 vs 1)\n",
    "feature_for_ttest = df.columns[0] if df.columns[0] != 'target_class' else df.columns[1]\n",
    "g0 = df[df[target_col] == 0][feature_for_ttest].values\n",
    "g1 = df[df[target_col] == 1][feature_for_ttest].values\n",
    "t_stat, p_val = ttest_ind(g0, g1, equal_var=False)\n",
    "print(f\"T‑test on '{feature_for_ttest}': t={t_stat:.4f}, p={p_val:.6f}\")\n",
    "print(\"Interpretation: p < 0.05 suggests the feature mean differs significantly between classes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2be024",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Tips\n",
    "- **Accuracy** can be misleading on imbalanced data; always check **Precision**, **Recall**, and **F1**.\n",
    "- **ROC AUC** summarizes ranking quality across thresholds; also consider **Precision‑Recall AUC** for heavy imbalance.\n",
    "- **Learning Curves** help diagnose **high bias (underfitting)** vs **high variance (overfitting)**.\n",
    "- **p‑values** from t‑tests are for feature‑wise mean differences (not model performance). For model comparison, prefer cross‑validated metrics and statistical tests designed for paired classifiers.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
