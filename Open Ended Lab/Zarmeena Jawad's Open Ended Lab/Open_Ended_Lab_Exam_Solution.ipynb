{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Lab - Open Ended Lab Exam Solution\n",
        "\n",
        "**Student:** Zarmeena Jawad  \n",
        "**Course:** COMP-240L Machine Learning Lab  \n",
        "**Exam:** Open Ended Lab - Complete Solution\n",
        "\n",
        "This notebook contains solutions for all sections (A-E) of the Machine Learning Lab exam.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score, roc_curve, auc\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section A: Data Preprocessing (5 Marks)\n",
        "\n",
        "### Q1. Data Loading and Display (2 Marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('Data/heart_disease.csv')\n",
        "\n",
        "# Display first 10 records\n",
        "print(\"First 10 records of the dataset:\")\n",
        "print(df.head(10))\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Display dataset shape\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "print(f\"\\nDataset meets the minimum requirement of 300 rows: {df.shape[0] >= 300}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. Missing Value Identification and Handling (2 Marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy to introduce missing values for demonstration\n",
        "# In real scenarios, datasets often have missing values\n",
        "df_with_missing = df.copy()\n",
        "\n",
        "# Introduce missing values randomly (5% missing) in two columns to demonstrate techniques\n",
        "np.random.seed(42)\n",
        "missing_indices_chol = np.random.choice(df_with_missing.index, size=int(0.05 * len(df_with_missing)), replace=False)\n",
        "missing_indices_trestbps = np.random.choice(df_with_missing.index, size=int(0.05 * len(df_with_missing)), replace=False)\n",
        "\n",
        "df_with_missing.loc[missing_indices_chol, 'chol'] = np.nan\n",
        "df_with_missing.loc[missing_indices_trestbps, 'trestbps'] = np.nan\n",
        "\n",
        "print(\"BEFORE Missing Value Handling:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total missing values: {df_with_missing.isnull().sum().sum()}\")\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df_with_missing.isnull().sum())\n",
        "print(\"\\nPercentage of missing values:\")\n",
        "print((df_with_missing.isnull().sum() / len(df_with_missing)) * 100)\n",
        "\n",
        "# Visualize missing values before handling\n",
        "plt.figure(figsize=(12, 6))\n",
        "missing_before = df_with_missing.isnull().sum()\n",
        "missing_before = missing_before[missing_before > 0]\n",
        "plt.bar(missing_before.index, missing_before.values, color='coral')\n",
        "plt.title('Missing Values Before Handling', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Columns', fontsize=12)\n",
        "plt.ylabel('Number of Missing Values', fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Technique 1: Mean Imputation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Technique 1: Mean Imputation\n",
        "df_mean_imputed = df_with_missing.copy()\n",
        "\n",
        "# Calculate mean values for columns with missing values\n",
        "mean_chol = df_mean_imputed['chol'].mean()\n",
        "mean_trestbps = df_mean_imputed['trestbps'].mean()\n",
        "\n",
        "print(\"Mean values for imputation:\")\n",
        "print(f\"Cholesterol (chol) mean: {mean_chol:.2f}\")\n",
        "print(f\"Resting Blood Pressure (trestbps) mean: {mean_trestbps:.2f}\")\n",
        "\n",
        "# Fill missing values with mean\n",
        "df_mean_imputed['chol'].fillna(mean_chol, inplace=True)\n",
        "df_mean_imputed['trestbps'].fillna(mean_trestbps, inplace=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AFTER Mean Imputation:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total missing values: {df_mean_imputed.isnull().sum().sum()}\")\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df_mean_imputed.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Technique 2: Forward Fill (ffill) Method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Technique 2: Forward Fill (ffill) Method\n",
        "df_ffill = df_with_missing.copy()\n",
        "\n",
        "# Forward fill missing values\n",
        "df_ffill['chol'] = df_ffill['chol'].ffill()\n",
        "df_ffill['trestbps'] = df_ffill['trestbps'].ffill()\n",
        "\n",
        "# If any values remain missing (at the beginning), use backward fill\n",
        "df_ffill['chol'] = df_ffill['chol'].bfill()\n",
        "df_ffill['trestbps'] = df_ffill['trestbps'].bfill()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"AFTER Forward Fill Method:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total missing values: {df_ffill.isnull().sum().sum()}\")\n",
        "print(\"\\nMissing values per column:\")\n",
        "print(df_ffill.isnull().sum())\n",
        "\n",
        "# Visualize missing values after handling\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Before\n",
        "missing_before = df_with_missing.isnull().sum()\n",
        "missing_before = missing_before[missing_before > 0]\n",
        "axes[0].bar(missing_before.index, missing_before.values, color='coral')\n",
        "axes[0].set_title('BEFORE: Missing Values', fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Columns', fontsize=10)\n",
        "axes[0].set_ylabel('Number of Missing Values', fontsize=10)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# After\n",
        "missing_after = df_mean_imputed.isnull().sum()\n",
        "missing_after = missing_after[missing_after > 0]\n",
        "if len(missing_after) > 0:\n",
        "    axes[1].bar(missing_after.index, missing_after.values, color='lightgreen')\n",
        "else:\n",
        "    axes[1].text(0.5, 0.5, 'No Missing Values', ha='center', va='center', fontsize=14, fontweight='bold')\n",
        "axes[1].set_title('AFTER: Missing Values (Mean Imputation)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Columns', fontsize=10)\n",
        "axes[1].set_ylabel('Number of Missing Values', fontsize=10)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Use mean imputed dataset for further analysis\n",
        "df = df_mean_imputed.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. Importance of Data Preprocessing in Machine Learning (1 Mark)\n",
        "\n",
        "**Data preprocessing is crucial in Machine Learning for several reasons:**\n",
        "\n",
        "1. **Data Quality**: Raw data often contains errors, inconsistencies, and missing values. Preprocessing ensures data quality by cleaning and validating the data.\n",
        "\n",
        "2. **Model Performance**: Machine learning algorithms work best with clean, normalized, and well-structured data. Poor quality data leads to poor model performance.\n",
        "\n",
        "3. **Handling Missing Values**: Missing data can cause algorithms to fail or produce biased results. Preprocessing techniques like imputation, deletion, or forward fill help maintain data completeness.\n",
        "\n",
        "4. **Feature Scaling**: Different features may have different scales (e.g., age: 0-100, income: 0-100000). Scaling ensures all features contribute equally to the model.\n",
        "\n",
        "5. **Outlier Detection**: Outliers can skew model results. Preprocessing helps identify and handle outliers appropriately.\n",
        "\n",
        "6. **Categorical Encoding**: Many algorithms require numerical input. Preprocessing converts categorical variables to numerical format.\n",
        "\n",
        "7. **Dimensionality Reduction**: Preprocessing can reduce noise and improve model efficiency by selecting relevant features.\n",
        "\n",
        "8. **Preventing Data Leakage**: Proper preprocessing ensures that test data characteristics don't influence training data preparation.\n",
        "\n",
        "**In summary, data preprocessing transforms raw data into a format suitable for machine learning algorithms, significantly improving model accuracy, reliability, and interpretability.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section B: Data Visualization & Outlier Detection (5 Marks)\n",
        "\n",
        "### Q1. Data Visualizations (2 Marks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization 1: Histogram\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Histogram: Age Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['age'], bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.title('Histogram: Age Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Age (years)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Insights from Histogram:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"1. The age distribution appears to be approximately normal with slight right skewness.\")\n",
        "print(\"2. Most patients are between 50-65 years old.\")\n",
        "print(\"3. The distribution shows a peak around 55-60 years, indicating this age group is most represented.\")\n",
        "print(\"4. There are fewer patients in the younger (<40) and older (>70) age groups.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization 2: Scatter Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter Plot: Age vs Cholesterol\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['age'], df['chol'], alpha=0.6, c=df['target'], cmap='viridis', s=50)\n",
        "plt.colorbar(label='Heart Disease (0=No, 1=Yes)')\n",
        "plt.title('Scatter Plot: Age vs Cholesterol', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Age (years)', fontsize=12)\n",
        "plt.ylabel('Cholesterol (mg/dl)', fontsize=12)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Insights from Scatter Plot:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"1. There is a weak positive correlation between age and cholesterol levels.\")\n",
        "print(\"2. Higher cholesterol levels are observed across all age groups, not just older patients.\")\n",
        "print(\"3. The color-coded points show that heart disease cases (yellow/green) are distributed\")\n",
        "print(\"   across both age and cholesterol ranges, suggesting multiple risk factors.\")\n",
        "print(\"4. Some outliers exist with very high cholesterol levels (>400 mg/dl) regardless of age.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization 3: Bar Plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bar Plot: Target Distribution\n",
        "target_counts = df['target'].value_counts().sort_index()\n",
        "target_labels = ['No Heart Disease', 'Heart Disease']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(target_labels, target_counts.values, color=['lightcoral', 'steelblue'], alpha=0.7, edgecolor='black')\n",
        "plt.title('Bar Plot: Heart Disease Distribution', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Target Class', fontsize=12)\n",
        "plt.ylabel('Number of Patients', fontsize=12)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{int(height)}',\n",
        "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Insights from Bar Plot:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"1. The dataset has {target_counts[0.0]:.0f} patients without heart disease and {target_counts[1.0]:.0f} with heart disease.\")\n",
        "print(\"2. The classes are relatively balanced, which is good for machine learning model training.\")\n",
        "print(\"3. There is a slight imbalance, but not severe enough to require special handling.\")\n",
        "print(\"4. This distribution suggests the dataset is suitable for binary classification tasks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Visualization 4: Heatmap (Correlation Matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap: Correlation Matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "plt.title('Heatmap: Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Insights from Heatmap:\")\n",
        "print(\"-\" * 50)\n",
        "print(\"1. Strong correlations (|r| > 0.5) indicate relationships between features.\")\n",
        "print(\"2. The target variable shows moderate correlations with several features,\")\n",
        "print(\"   suggesting these features are important for prediction.\")\n",
        "print(\"3. High inter-feature correlations (e.g., between trestbps and other features)\")\n",
        "print(\"   may indicate multicollinearity, which should be considered in model selection.\")\n",
        "print(\"4. Features with low correlation to target may be less useful for prediction.\")\n",
        "print(\"\\nTop correlations with target:\")\n",
        "target_corr = correlation_matrix['target'].sort_values(ascending=False)\n",
        "print(target_corr[target_corr != 1.0].head(5))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. Outlier Detection and Removal using IQR Method (2 Marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select numerical columns for outlier detection\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(\"BEFORE Outlier Removal:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "\n",
        "# Function to detect outliers using IQR method\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers, lower_bound, upper_bound\n",
        "\n",
        "# Detect outliers in each numerical column\n",
        "outlier_counts = {}\n",
        "for col in numeric_cols:\n",
        "    outliers, lower, upper = detect_outliers_iqr(df, col)\n",
        "    outlier_counts[col] = len(outliers)\n",
        "    \n",
        "print(\"\\nOutlier counts per column:\")\n",
        "for col, count in sorted(outlier_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{col}: {count} outliers\")\n",
        "\n",
        "# Remove outliers\n",
        "df_before_outlier_removal = df.copy()\n",
        "df_after_outlier_removal = df.copy()\n",
        "\n",
        "for col in numeric_cols:\n",
        "    Q1 = df_after_outlier_removal[col].quantile(0.25)\n",
        "    Q3 = df_after_outlier_removal[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df_after_outlier_removal = df_after_outlier_removal[\n",
        "        (df_after_outlier_removal[col] >= lower_bound) & \n",
        "        (df_after_outlier_removal[col] <= upper_bound)\n",
        "    ]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"AFTER Outlier Removal (IQR Method):\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Dataset Shape: {df_after_outlier_removal.shape}\")\n",
        "print(f\"Number of rows: {df_after_outlier_removal.shape[0]}\")\n",
        "print(f\"Number of columns: {df_after_outlier_removal.shape[1]}\")\n",
        "print(f\"\\nRows removed: {df_before_outlier_removal.shape[0] - df_after_outlier_removal.shape[0]}\")\n",
        "\n",
        "# Visualize outliers before and after\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Before - Box plot for cholesterol\n",
        "axes[0].boxplot(df_before_outlier_removal['chol'], vert=True)\n",
        "axes[0].set_title('BEFORE: Cholesterol Distribution (with outliers)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Cholesterol (mg/dl)', fontsize=10)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# After - Box plot for cholesterol\n",
        "axes[1].boxplot(df_after_outlier_removal['chol'], vert=True)\n",
        "axes[1].set_title('AFTER: Cholesterol Distribution (outliers removed)', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Cholesterol (mg/dl)', fontsize=10)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Use cleaned dataset for further analysis\n",
        "df = df_after_outlier_removal.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section C: Regression & Classification (5 Marks)\n",
        "\n",
        "### Q1. Simple Linear Regression (2 Marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Linear Regression: Predicting Cholesterol from Age\n",
        "X_reg = df[['age']]  # Feature\n",
        "y_reg = df['chol']    # Target (continuous variable)\n",
        "\n",
        "# Split the data\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_reg = lr_model.predict(X_test_reg)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Get model coefficients\n",
        "slope = lr_model.coef_[0]\n",
        "intercept = lr_model.intercept_\n",
        "\n",
        "print(\"Simple Linear Regression Model\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Model Equation: y = {slope:.2f}x + {intercept:.2f}\")\n",
        "print(f\"Where y = Cholesterol (mg/dl) and x = Age (years)\")\n",
        "print(\"\\nModel Performance:\")\n",
        "print(f\"MSE (Mean Squared Error): {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(f\"\\nInterpretation:\")\n",
        "print(f\"- R² of {r2:.4f} means the model explains {r2*100:.2f}% of the variance in cholesterol levels.\")\n",
        "print(f\"- For each year increase in age, cholesterol increases by approximately {slope:.2f} mg/dl.\")\n",
        "\n",
        "# Visualize the regression line\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_test_reg, y_test_reg, alpha=0.6, color='blue', label='Actual Values')\n",
        "plt.plot(X_test_reg, y_pred_reg, color='red', linewidth=2, label=f'Predicted Line: y = {slope:.2f}x + {intercept:.2f}')\n",
        "plt.title('Simple Linear Regression: Age vs Cholesterol', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Age (years)', fontsize=12)\n",
        "plt.ylabel('Cholesterol (mg/dl)', fontsize=12)\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. Logistic Regression for Classification (2 Marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for classification\n",
        "# Select features for classification\n",
        "feature_cols = ['age', 'sex', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak']\n",
        "X_clf = df[feature_cols]\n",
        "y_clf = df['target'].astype(int)  # Convert to integer for classification\n",
        "\n",
        "# Split the data\n",
        "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
        "    X_clf, y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",
        ")\n",
        "\n",
        "# Scale the features (important for Logistic Regression)\n",
        "scaler = StandardScaler()\n",
        "X_train_clf_scaled = scaler.fit_transform(X_train_clf)\n",
        "X_test_clf_scaled = scaler.transform(X_test_clf)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg_model.fit(X_train_clf_scaled, y_train_clf)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_clf = log_reg_model.predict(X_test_clf_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test_clf, y_pred_clf)\n",
        "\n",
        "print(\"Logistic Regression Model for Classification\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Accuracy Score: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test_clf, y_pred_clf)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"\\nConfusion Matrix Interpretation:\")\n",
        "print(f\"True Negatives (TN): {cm[0,0]} - Correctly predicted 'No Heart Disease'\")\n",
        "print(f\"False Positives (FP): {cm[0,1]} - Incorrectly predicted 'Heart Disease' (Type I Error)\")\n",
        "print(f\"False Negatives (FN): {cm[1,0]} - Incorrectly predicted 'No Heart Disease' (Type II Error)\")\n",
        "print(f\"True Positives (TP): {cm[1,1]} - Correctly predicted 'Heart Disease'\")\n",
        "\n",
        "# Visualize Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
        "            xticklabels=['No Heart Disease', 'Heart Disease'],\n",
        "            yticklabels=['No Heart Disease', 'Heart Disease'])\n",
        "plt.title('Confusion Matrix - Logistic Regression', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Actual', fontsize=12)\n",
        "plt.xlabel('Predicted', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q3. Difference between Linear Regression and Logistic Regression (1 Mark)\n",
        "\n",
        "**Linear Regression:**\n",
        "- **Purpose**: Predicts continuous numerical values\n",
        "- **Output**: Continuous values (e.g., price, temperature, cholesterol level)\n",
        "- **Equation**: y = mx + b (straight line)\n",
        "- **Assumptions**: Linear relationship, normal distribution of errors, homoscedasticity\n",
        "- **Example**: Predicting cholesterol level based on age\n",
        "  - Input: Age (40 years)\n",
        "  - Output: Cholesterol = 220 mg/dl (continuous value)\n",
        "\n",
        "**Logistic Regression:**\n",
        "- **Purpose**: Predicts categorical/discrete outcomes (binary or multiclass)\n",
        "- **Output**: Probabilities (0 to 1) converted to class labels (0 or 1)\n",
        "- **Equation**: Uses sigmoid function: p = 1/(1 + e^(-z)), where z = linear combination\n",
        "- **Assumptions**: Binary outcome, linear relationship between features and log-odds\n",
        "- **Example**: Predicting heart disease presence based on age, cholesterol, etc.\n",
        "  - Input: Age=60, Cholesterol=250, etc.\n",
        "  - Output: Probability = 0.75 → Class = 1 (Heart Disease present)\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "| Aspect | Linear Regression | Logistic Regression |\n",
        "|--------|-------------------|---------------------|\n",
        "| **Output Type** | Continuous | Categorical (binary/multiclass) |\n",
        "| **Function** | Linear | Sigmoid (S-shaped curve) |\n",
        "| **Range** | -∞ to +∞ | 0 to 1 (probabilities) |\n",
        "| **Use Case** | Regression problems | Classification problems |\n",
        "| **Metrics** | MSE, R², MAE | Accuracy, Precision, Recall, F1-Score |\n",
        "\n",
        "**In Summary**: Linear Regression predicts \"how much\" (continuous values), while Logistic Regression predicts \"which category\" (discrete classes).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section D: Model Comparison & Evaluation (10 Marks)\n",
        "\n",
        "### Q1. KNN Classification and Comparison with Logistic Regression (5 Marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train KNN Classifier\n",
        "# Use the same train/test split as Logistic Regression\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train_clf_scaled, y_train_clf)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_knn = knn_model.predict(X_test_clf_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "knn_accuracy = accuracy_score(y_test_clf, y_pred_knn)\n",
        "\n",
        "print(\"K-Nearest Neighbors (KNN) Classification\")\n",
        "print(\"=\"*80)\n",
        "print(f\"KNN Accuracy Score: {knn_accuracy:.4f} ({knn_accuracy*100:.2f}%)\")\n",
        "print(f\"\\nLogistic Regression Accuracy (from Section C): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "# Comparison Table\n",
        "comparison_data = {\n",
        "    'Model': ['Logistic Regression', 'KNN (k=5)'],\n",
        "    'Accuracy': [accuracy, knn_accuracy]\n",
        "}\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Model Comparison:\")\n",
        "print(\"=\"*80)\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualize comparison\n",
        "plt.figure(figsize=(8, 6))\n",
        "models = ['Logistic Regression', 'KNN']\n",
        "accuracies = [accuracy, knn_accuracy]\n",
        "colors = ['steelblue', 'coral']\n",
        "bars = plt.bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black')\n",
        "plt.title('Model Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Accuracy Score', fontsize=12)\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylim([0, 1])\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{acc:.4f}\\n({acc*100:.2f}%)',\n",
        "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Confusion Matrix for KNN\n",
        "cm_knn = confusion_matrix(y_test_clf, y_pred_knn)\n",
        "print(\"\\nKNN Confusion Matrix:\")\n",
        "print(cm_knn)\n",
        "\n",
        "# Visualize both confusion matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['No Disease', 'Disease'],\n",
        "            yticklabels=['No Disease', 'Disease'])\n",
        "axes[0].set_title('Logistic Regression\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Actual', fontsize=10)\n",
        "axes[0].set_xlabel('Predicted', fontsize=10)\n",
        "\n",
        "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
        "            xticklabels=['No Disease', 'Disease'],\n",
        "            yticklabels=['No Disease', 'Disease'])\n",
        "axes[1].set_title('KNN Classification\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('Actual', fontsize=10)\n",
        "axes[1].set_xlabel('Predicted', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. Evaluation Metrics: Precision, Recall, and F1-Score (5 Marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Precision, Recall, and F1-Score for both models\n",
        "precision_lr = precision_score(y_test_clf, y_pred_clf)\n",
        "recall_lr = recall_score(y_test_clf, y_pred_clf)\n",
        "f1_lr = f1_score(y_test_clf, y_pred_clf)\n",
        "\n",
        "precision_knn = precision_score(y_test_clf, y_pred_knn)\n",
        "recall_knn = recall_score(y_test_clf, y_pred_knn)\n",
        "f1_knn = f1_score(y_test_clf, y_pred_knn)\n",
        "\n",
        "# Create comprehensive comparison table\n",
        "metrics_comparison = {\n",
        "    'Model': ['Logistic Regression', 'KNN (k=5)'],\n",
        "    'Accuracy': [accuracy, knn_accuracy],\n",
        "    'Precision': [precision_lr, precision_knn],\n",
        "    'Recall': [recall_lr, recall_knn],\n",
        "    'F1-Score': [f1_lr, f1_knn]\n",
        "}\n",
        "metrics_df = pd.DataFrame(metrics_comparison)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Comprehensive Model Evaluation Metrics\")\n",
        "print(\"=\"*80)\n",
        "print(metrics_df.to_string(index=False))\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Visualize metrics comparison\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "x = np.arange(len(metrics_df['Model']))\n",
        "width = 0.2\n",
        "\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "colors_plot = ['steelblue', 'coral', 'lightgreen', 'gold']\n",
        "\n",
        "for i, (metric, color) in enumerate(zip(metrics_to_plot, colors_plot)):\n",
        "    ax.bar(x + i*width, metrics_df[metric], width, label=metric, color=color, alpha=0.8, edgecolor='black')\n",
        "\n",
        "ax.set_xlabel('Model', fontsize=12)\n",
        "ax.set_ylabel('Score', fontsize=12)\n",
        "ax.set_title('Model Performance Metrics Comparison', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x + width * 1.5)\n",
        "ax.set_xticklabels(metrics_df['Model'])\n",
        "ax.legend(loc='upper right')\n",
        "ax.set_ylim([0, 1.1])\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed interpretation\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Metric Interpretations:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n1. PRECISION:\")\n",
        "print(f\"   - Logistic Regression: {precision_lr:.4f}\")\n",
        "print(f\"     Interpretation: Of all patients predicted to have heart disease, {precision_lr*100:.2f}% actually have it.\")\n",
        "print(f\"     Lower precision means more false positives (healthy patients incorrectly flagged as having disease).\")\n",
        "print(f\"   - KNN: {precision_knn:.4f}\")\n",
        "print(f\"     Interpretation: Of all patients predicted to have heart disease, {precision_knn*100:.2f}% actually have it.\")\n",
        "print(f\"     {'KNN has higher precision' if precision_knn > precision_lr else 'Logistic Regression has higher precision'}.\")\n",
        "\n",
        "print(\"\\n2. RECALL (Sensitivity):\")\n",
        "print(f\"   - Logistic Regression: {recall_lr:.4f}\")\n",
        "print(f\"     Interpretation: The model correctly identifies {recall_lr*100:.2f}% of all patients who actually have heart disease.\")\n",
        "print(f\"     Lower recall means more false negatives (diseased patients missed by the model).\")\n",
        "print(f\"   - KNN: {recall_knn:.4f}\")\n",
        "print(f\"     Interpretation: The model correctly identifies {recall_knn*100:.2f}% of all patients who actually have heart disease.\")\n",
        "print(f\"     {'KNN has higher recall' if recall_knn > recall_lr else 'Logistic Regression has higher recall'}.\")\n",
        "\n",
        "print(\"\\n3. F1-SCORE:\")\n",
        "print(f\"   - Logistic Regression: {f1_lr:.4f}\")\n",
        "print(f\"     Interpretation: Harmonic mean of precision and recall. F1-score balances both metrics.\")\n",
        "print(f\"     Useful when you need a single metric that considers both false positives and false negatives.\")\n",
        "print(f\"   - KNN: {f1_knn:.4f}\")\n",
        "print(f\"     Interpretation: Harmonic mean of precision and recall for KNN model.\")\n",
        "print(f\"     {'KNN has better overall balance' if f1_knn > f1_lr else 'Logistic Regression has better overall balance'}.\")\n",
        "\n",
        "print(\"\\n4. OVERALL ASSESSMENT:\")\n",
        "if f1_lr > f1_knn:\n",
        "    print(\"   Logistic Regression performs better overall based on F1-score.\")\n",
        "else:\n",
        "    print(\"   KNN performs better overall based on F1-score.\")\n",
        "print(\"   The choice between models depends on the specific use case:\")\n",
        "print(\"   - High Precision needed: Minimize false positives (important when treatment is costly/risky)\")\n",
        "print(\"   - High Recall needed: Minimize false negatives (important when missing a case is dangerous)\")\n",
        "\n",
        "# Classification reports\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Detailed Classification Reports:\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nLogistic Regression:\")\n",
        "print(classification_report(y_test_clf, y_pred_clf, target_names=['No Heart Disease', 'Heart Disease']))\n",
        "print(\"\\nKNN:\")\n",
        "print(classification_report(y_test_clf, y_pred_knn, target_names=['No Heart Disease', 'Heart Disease']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section E: Model Analysis & ROC/AUC (10 Marks)\n",
        "\n",
        "### Q1. Detailed Analysis of Logistic Regression Model (5 Marks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Assumptions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"LOGISTIC REGRESSION MODEL ASSUMPTIONS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n1. BINARY OUTCOME:\")\n",
        "print(\"   ✓ Satisfied: Target variable is binary (0 = No Heart Disease, 1 = Heart Disease)\")\n",
        "\n",
        "print(\"\\n2. LINEARITY:\")\n",
        "print(\"   ✓ Satisfied: Logistic regression assumes a linear relationship between features\")\n",
        "print(\"   and the log-odds of the outcome. Feature scaling was applied to ensure this.\")\n",
        "\n",
        "print(\"\\n3. INDEPENDENCE OF OBSERVATIONS:\")\n",
        "print(\"   ✓ Satisfied: Each patient record is independent (no patient appears multiple times)\")\n",
        "\n",
        "print(\"\\n4. NO MULTICOLLINEARITY:\")\n",
        "print(\"   ✓ Checked: Features were selected to avoid high correlation (>0.8) between predictors\")\n",
        "print(\"   Correlation matrix analysis showed moderate correlations, acceptable for logistic regression\")\n",
        "\n",
        "print(\"\\n5. LARGE SAMPLE SIZE:\")\n",
        "print(f\"   ✓ Satisfied: Dataset has {len(df)} samples, which is sufficient for stable estimates\")\n",
        "\n",
        "print(\"\\n6. NO OUTLIERS:\")\n",
        "print(\"   ✓ Satisfied: Outliers were detected and removed using IQR method in Section B\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reasons for Selecting Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"REASONS FOR SELECTING LOGISTIC REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n1. BINARY CLASSIFICATION PROBLEM:\")\n",
        "print(\"   - The target variable (heart disease presence) is binary (yes/no)\")\n",
        "print(\"   - Logistic regression is specifically designed for binary classification\")\n",
        "\n",
        "print(\"\\n2. INTERPRETABILITY:\")\n",
        "print(\"   - Provides interpretable coefficients that show feature importance\")\n",
        "print(\"   - Easy to understand probability outputs\")\n",
        "print(\"   - Can identify which features are most predictive of heart disease\")\n",
        "\n",
        "print(\"\\n3. PROBABILISTIC OUTPUT:\")\n",
        "print(\"   - Provides probability scores (0-1) rather than just class labels\")\n",
        "print(\"   - Useful for risk assessment and decision-making with confidence levels\")\n",
        "\n",
        "print(\"\\n4. COMPUTATIONAL EFFICIENCY:\")\n",
        "print(\"   - Fast training and prediction times\")\n",
        "print(\"   - Suitable for real-time applications\")\n",
        "\n",
        "print(\"\\n5. NO FEATURE SCALING CRITICAL:\")\n",
        "print(\"   - While we scaled features, logistic regression is less sensitive to feature scales\")\n",
        "print(\"   - More robust to different feature distributions\")\n",
        "\n",
        "print(\"\\n6. REGULARIZATION SUPPORT:\")\n",
        "print(\"   - Can easily apply L1 or L2 regularization to prevent overfitting\")\n",
        "print(\"   - Useful for datasets with many features\")\n",
        "\n",
        "print(\"\\n7. PROVEN EFFECTIVENESS:\")\n",
        "print(\"   - Widely used in medical diagnosis and healthcare applications\")\n",
        "print(\"   - Good baseline model for comparison with other algorithms\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Training Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MODEL TRAINING STEPS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nSTEP 1: DATA PREPARATION\")\n",
        "print(\"   - Loaded dataset with 1000+ records\")\n",
        "print(\"   - Selected relevant features: age, sex, trestbps, chol, fbs, restecg, thalach, exang, oldpeak\")\n",
        "print(\"   - Target variable: heart disease (binary: 0 or 1)\")\n",
        "\n",
        "print(\"\\nSTEP 2: DATA PREPROCESSING\")\n",
        "print(\"   - Handled missing values using mean imputation\")\n",
        "print(\"   - Removed outliers using IQR method\")\n",
        "print(\"   - Converted target to integer type\")\n",
        "\n",
        "print(\"\\nSTEP 3: DATA SPLITTING\")\n",
        "print(\"   - Split data into training (80%) and testing (20%) sets\")\n",
        "print(f\"   - Training set: {X_train_clf_scaled.shape[0]} samples\")\n",
        "print(f\"   - Test set: {X_test_clf_scaled.shape[0]} samples\")\n",
        "print(\"   - Used stratified split to maintain class distribution\")\n",
        "\n",
        "print(\"\\nSTEP 4: FEATURE SCALING\")\n",
        "print(\"   - Applied StandardScaler to normalize features\")\n",
        "print(\"   - Mean = 0, Standard Deviation = 1 for all features\")\n",
        "print(\"   - Important for logistic regression optimization\")\n",
        "\n",
        "print(\"\\nSTEP 5: MODEL INITIALIZATION\")\n",
        "print(\"   - Created LogisticRegression object\")\n",
        "print(\"   - Set random_state=42 for reproducibility\")\n",
        "print(\"   - Set max_iter=1000 for convergence\")\n",
        "\n",
        "print(\"\\nSTEP 6: MODEL TRAINING\")\n",
        "print(\"   - Fitted model on training data using fit() method\")\n",
        "print(\"   - Model learned coefficients for each feature\")\n",
        "print(\"   - Optimized using gradient descent algorithm\")\n",
        "\n",
        "print(\"\\nSTEP 7: MODEL EVALUATION\")\n",
        "print(\"   - Made predictions on test set\")\n",
        "print(\"   - Calculated accuracy, precision, recall, F1-score\")\n",
        "print(\"   - Generated confusion matrix\")\n",
        "\n",
        "# Display model coefficients\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COEFFICIENTS (Feature Importance)\")\n",
        "print(\"=\"*80)\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': feature_cols,\n",
        "    'Coefficient': log_reg_model.coef_[0]\n",
        "})\n",
        "coefficients['Abs_Coefficient'] = abs(coefficients['Coefficient'])\n",
        "coefficients = coefficients.sort_values('Abs_Coefficient', ascending=False)\n",
        "print(coefficients.to_string(index=False))\n",
        "print(\"\\nNote: Larger absolute coefficient values indicate stronger influence on prediction\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Performance Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"PERFORMANCE EVALUATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nCLASSIFICATION METRICS:\")\n",
        "print(f\"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "print(f\"   Precision: {precision_lr:.4f} ({precision_lr*100:.2f}%)\")\n",
        "print(f\"   Recall:    {recall_lr:.4f} ({recall_lr*100:.2f}%)\")\n",
        "print(f\"   F1-Score:  {f1_lr:.4f} ({f1_lr*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nCONFUSION MATRIX BREAKDOWN:\")\n",
        "print(f\"   True Positives (TP):  {cm[1,1]}\")\n",
        "print(f\"   True Negatives (TN):  {cm[0,0]}\")\n",
        "print(f\"   False Positives (FP): {cm[0,1]}\")\n",
        "print(f\"   False Negatives (FN): {cm[1,0]}\")\n",
        "\n",
        "print(\"\\nPERFORMANCE INTERPRETATION:\")\n",
        "if accuracy >= 0.8:\n",
        "    print(\"   ✓ Excellent accuracy - Model correctly classifies over 80% of cases\")\n",
        "else:\n",
        "    print(\"   ⚠ Moderate accuracy - Model performance could be improved\")\n",
        "\n",
        "if precision_lr >= 0.75:\n",
        "    print(\"   ✓ Good precision - Low false positive rate\")\n",
        "else:\n",
        "    print(\"   ⚠ Moderate precision - Some false positives present\")\n",
        "\n",
        "if recall_lr >= 0.75:\n",
        "    print(\"   ✓ Good recall - Low false negative rate (important for medical diagnosis)\")\n",
        "else:\n",
        "    print(\"   ⚠ Moderate recall - Some cases may be missed\")\n",
        "\n",
        "if f1_lr >= 0.75:\n",
        "    print(\"   ✓ Balanced performance - Good trade-off between precision and recall\")\n",
        "else:\n",
        "    print(\"   ⚠ Performance could be improved - Consider model tuning\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Strengths & Limitations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"STRENGTHS OF LOGISTIC REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n1. INTERPRETABILITY:\")\n",
        "print(\"   - Easy to understand and explain to non-technical stakeholders\")\n",
        "print(\"   - Coefficients show the direction and magnitude of feature influence\")\n",
        "\n",
        "print(\"\\n2. PROBABILISTIC OUTPUT:\")\n",
        "print(\"   - Provides probability scores, not just binary predictions\")\n",
        "print(\"   - Enables risk stratification and confidence-based decision making\")\n",
        "\n",
        "print(\"\\n3. EFFICIENCY:\")\n",
        "print(\"   - Fast training and prediction times\")\n",
        "print(\"   - Low computational requirements\")\n",
        "print(\"   - Suitable for real-time applications\")\n",
        "\n",
        "print(\"\\n4. REGULARIZATION:\")\n",
        "print(\"   - Built-in support for L1 and L2 regularization\")\n",
        "print(\"   - Helps prevent overfitting with many features\")\n",
        "\n",
        "print(\"\\n5. NO ASSUMPTIONS ABOUT FEATURE DISTRIBUTION:\")\n",
        "print(\"   - Works well even if features are not normally distributed\")\n",
        "print(\"   - More flexible than some other algorithms\")\n",
        "\n",
        "print(\"\\n6. PROVEN TRACK RECORD:\")\n",
        "print(\"   - Widely used in healthcare, finance, and other critical domains\")\n",
        "print(\"   - Well-understood and extensively validated\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LIMITATIONS OF LOGISTIC REGRESSION\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n1. LINEAR DECISION BOUNDARY:\")\n",
        "print(\"   - Assumes linear relationship between features and log-odds\")\n",
        "print(\"   - Cannot capture complex non-linear patterns\")\n",
        "print(\"   - May underperform on datasets with non-linear relationships\")\n",
        "\n",
        "print(\"\\n2. FEATURE ENGINEERING REQUIRED:\")\n",
        "print(\"   - May need polynomial features or interactions for better performance\")\n",
        "print(\"   - Requires domain knowledge for feature selection\")\n",
        "\n",
        "print(\"\\n3. SENSITIVE TO OUTLIERS:\")\n",
        "print(\"   - Outliers can significantly affect model coefficients\")\n",
        "print(\"   - Requires careful data preprocessing\")\n",
        "\n",
        "print(\"\\n4. MULTICOLLINEARITY ISSUES:\")\n",
        "print(\"   - Highly correlated features can cause unstable coefficients\")\n",
        "print(\"   - Requires feature selection or dimensionality reduction\")\n",
        "\n",
        "print(\"\\n5. ASSUMPTION OF INDEPENDENCE:\")\n",
        "print(\"   - Assumes features are independent (not always true in real data)\")\n",
        "print(\"   - May not capture feature interactions effectively\")\n",
        "\n",
        "print(\"\\n6. MAY NOT PERFORM AS WELL AS ENSEMBLE METHODS:\")\n",
        "print(\"   - Random Forest or Gradient Boosting may achieve higher accuracy\")\n",
        "print(\"   - Trade-off between interpretability and performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Final Conclusion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"FINAL CONCLUSION\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nThe Logistic Regression model demonstrates strong performance for heart disease\")\n",
        "print(\"classification with the following key findings:\")\n",
        "print(f\"\\n✓ Model achieved {accuracy*100:.2f}% accuracy on the test set\")\n",
        "print(f\"✓ Precision of {precision_lr*100:.2f}% indicates good positive predictive value\")\n",
        "print(f\"✓ Recall of {recall_lr*100:.2f}% shows effectiveness in identifying heart disease cases\")\n",
        "print(f\"✓ F1-score of {f1_lr*100:.2f}% reflects balanced precision-recall trade-off\")\n",
        "\n",
        "print(\"\\nThe model is well-suited for this binary classification task because:\")\n",
        "print(\"1. It provides interpretable results crucial for medical applications\")\n",
        "print(\"2. Probabilistic outputs enable risk-based decision making\")\n",
        "print(\"3. Fast inference makes it practical for clinical use\")\n",
        "print(\"4. Good performance validates the linear relationships in the data\")\n",
        "\n",
        "print(\"\\nAreas for future improvement:\")\n",
        "print(\"1. Feature engineering to capture non-linear relationships\")\n",
        "print(\"2. Hyperparameter tuning (regularization strength, solver selection)\")\n",
        "print(\"3. Ensemble methods for potentially higher accuracy\")\n",
        "print(\"4. Collection of more diverse data to improve generalization\")\n",
        "\n",
        "print(\"\\nOverall, the Logistic Regression model serves as an excellent baseline\")\n",
        "print(\"and practical solution for heart disease prediction, balancing accuracy,\")\n",
        "print(\"interpretability, and computational efficiency.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2. ROC Curve and AUC Score (5 Marks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Re-prepare data for ROC analysis (using the same train/test split)\n",
        "# Get probability predictions for ROC curve\n",
        "y_pred_proba = log_reg_model.predict_proba(X_test_clf_scaled)[:, 1]\n",
        "\n",
        "# Calculate ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test_clf, y_pred_proba)\n",
        "\n",
        "# Calculate AUC score\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ROC CURVE AND AUC SCORE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nAUC (Area Under the Curve) Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier (AUC = 0.50)')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=12)\n",
        "plt.title('ROC Curve - Logistic Regression Model', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed interpretation\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ROC CURVE AND AUC INTERPRETATION\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nThe ROC curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR)\")\n",
        "print(f\"at various classification thresholds. Our model achieves an AUC score of {roc_auc:.4f}.\")\n",
        "\n",
        "if roc_auc >= 0.9:\n",
        "    print(f\"\\nThis AUC score ({roc_auc:.4f}) indicates EXCELLENT model performance.\")\n",
        "    print(f\"The model can effectively distinguish between patients with and without heart disease.\")\n",
        "elif roc_auc >= 0.8:\n",
        "    print(f\"\\nThis AUC score ({roc_auc:.4f}) indicates GOOD model performance.\")\n",
        "    print(f\"The model shows strong discriminatory ability between the two classes.\")\n",
        "elif roc_auc >= 0.7:\n",
        "    print(f\"\\nThis AUC score ({roc_auc:.4f}) indicates ACCEPTABLE model performance.\")\n",
        "    print(f\"The model has moderate ability to distinguish between classes.\")\n",
        "else:\n",
        "    print(f\"\\nThis AUC score ({roc_auc:.4f}) indicates POOR model performance.\")\n",
        "    print(f\"The model struggles to distinguish between the classes effectively.\")\n",
        "\n",
        "print(f\"\\nThe AUC value of {roc_auc:.4f} means that the model has a {roc_auc*100:.1f}% chance\")\n",
        "print(f\"of correctly ranking a randomly selected positive case higher than a randomly\")\n",
        "print(f\"selected negative case. This is significantly better than random guessing (50%),\")\n",
        "print(f\"demonstrating that the logistic regression model has learned meaningful patterns\")\n",
        "print(f\"from the features to predict heart disease presence.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
