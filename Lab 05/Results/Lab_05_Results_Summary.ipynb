{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 05: Logistic Regression Analysis - Results Summary\n",
        "\n",
        "**Department of Electrical and Computer Engineering**  \n",
        "**Pak-Austria Fachhochschule: Institute of Applied Sciences & Technology**  \n",
        "**Subject: Machine Learning**  \n",
        "**Subject Teacher: Dr. Abid Ali**  \n",
        "**Lab Supervisor: Miss. Sana Saleem**\n",
        "\n",
        "## Executive Summary\n",
        "\n",
        "This notebook presents a comprehensive summary of the logistic regression analysis results for heart disease prediction. It includes all key metrics, visualizations, and performance analysis in a format suitable for presentation and reporting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
        "                           classification_report, roc_auc_score)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"Set2\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('../Data/heart_disease_dataset.csv')\n",
        "\n",
        "# Prepare data\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression model\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
        "log_reg.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = log_reg.predict(X_test_scaled)\n",
        "y_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Training samples: {X_train.shape[0]}\")\n",
        "print(f\"Test samples: {X_test.shape[0]}\")\n",
        "print(f\"Target distribution: {y.value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Performance Metrics Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate all performance metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "specificity = tn / (tn + fp)\n",
        "sensitivity = tp / (tp + fn)\n",
        "\n",
        "# Cross-validation\n",
        "cv_scores = cross_val_score(log_reg, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "cv_precision = cross_val_score(log_reg, X_train_scaled, y_train, cv=5, scoring='precision')\n",
        "cv_recall = cross_val_score(log_reg, X_train_scaled, y_train, cv=5, scoring='recall')\n",
        "cv_f1 = cross_val_score(log_reg, X_train_scaled, y_train, cv=5, scoring='f1')\n",
        "\n",
        "# Create performance summary table\n",
        "performance_data = {\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'Specificity', 'Sensitivity'],\n",
        "    'Test Score': [f\"{accuracy:.4f}\", f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\", \n",
        "                   f\"{roc_auc:.4f}\", f\"{specificity:.4f}\", f\"{sensitivity:.4f}\"],\n",
        "    'CV Mean': [f\"{cv_scores.mean():.4f}\", f\"{cv_precision.mean():.4f}\", f\"{cv_recall.mean():.4f}\", \n",
        "                f\"{cv_f1.mean():.4f}\", \"N/A\", \"N/A\", \"N/A\"],\n",
        "    'CV Std': [f\"±{cv_scores.std():.4f}\", f\"±{cv_precision.std():.4f}\", f\"±{cv_recall.std():.4f}\", \n",
        "               f\"±{cv_f1.std():.4f}\", \"N/A\", \"N/A\", \"N/A\"]\n",
        "}\n",
        "\n",
        "performance_df = pd.DataFrame(performance_data)\n",
        "print(\"=== MODEL PERFORMANCE SUMMARY ===\")\n",
        "print(performance_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\n=== CONFUSION MATRIX ===\")\n",
        "print(f\"True Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "print(f\"True Positives: {tp}\")\n",
        "\n",
        "print(f\"\\n=== DETAILED CLASSIFICATION REPORT ===\")\n",
        "print(classification_report(y_test, y_pred, target_names=['No Heart Disease', 'Heart Disease']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Key Visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualization dashboard\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "# 1. Confusion Matrix\n",
        "plt.subplot(3, 3, 1)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['No Heart Disease', 'Heart Disease'],\n",
        "            yticklabels=['No Heart Disease', 'Heart Disease'])\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# 2. ROC Curve\n",
        "plt.subplot(3, 3, 2)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Precision-Recall Curve\n",
        "plt.subplot(3, 3, 3)\n",
        "precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_test, y_pred_proba)\n",
        "pr_auc = auc(recall_curve, precision_curve)\n",
        "plt.plot(recall_curve, precision_curve, color='darkorange', lw=2, label=f'PR curve (AUC = {pr_auc:.4f})')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Performance Metrics Bar Chart\n",
        "plt.subplot(3, 3, 4)\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
        "scores = [accuracy, precision, recall, f1, roc_auc]\n",
        "bars = plt.bar(metrics, scores, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])\n",
        "plt.title('Performance Metrics', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "for i, bar in enumerate(bars):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{scores[i]:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# 5. Cross-Validation Scores\n",
        "plt.subplot(3, 3, 5)\n",
        "cv_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "cv_means = [cv_scores.mean(), cv_precision.mean(), cv_recall.mean(), cv_f1.mean()]\n",
        "cv_stds = [cv_scores.std(), cv_precision.std(), cv_recall.std(), cv_f1.std()]\n",
        "x_pos = np.arange(len(cv_metrics))\n",
        "bars = plt.bar(x_pos, cv_means, yerr=cv_stds, capsize=5, color=['skyblue', 'lightgreen', 'lightcoral', 'gold'])\n",
        "plt.title('Cross-Validation Scores', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Score')\n",
        "plt.xticks(x_pos, cv_metrics, rotation=45)\n",
        "plt.ylim(0, 1)\n",
        "for i, (mean, std) in enumerate(zip(cv_means, cv_stds)):\n",
        "    plt.text(i, mean + std + 0.01, f'{mean:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 6. Feature Importance\n",
        "plt.subplot(3, 3, 6)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X.columns,\n",
        "    'coefficient': log_reg.coef_[0],\n",
        "    'abs_coefficient': np.abs(log_reg.coef_[0])\n",
        "}).sort_values('abs_coefficient', ascending=True)\n",
        "\n",
        "colors = ['red' if x < 0 else 'green' for x in feature_importance['coefficient']]\n",
        "bars = plt.barh(range(len(feature_importance)), feature_importance['coefficient'], color=colors)\n",
        "plt.title('Feature Importance (Coefficients)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.yticks(range(len(feature_importance)), feature_importance['feature'])\n",
        "plt.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "# 7. Target Distribution\n",
        "plt.subplot(3, 3, 7)\n",
        "target_counts = y.value_counts()\n",
        "plt.pie(target_counts.values, labels=['No Heart Disease', 'Heart Disease'], \n",
        "        autopct='%1.1f%%', colors=['lightcoral', 'lightblue'], startangle=90)\n",
        "plt.title('Target Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 8. Prediction Probability Distribution\n",
        "plt.subplot(3, 3, 8)\n",
        "plt.hist(y_pred_proba[y_test == 0], bins=20, alpha=0.7, label='No Heart Disease', color='lightcoral')\n",
        "plt.hist(y_pred_proba[y_test == 1], bins=20, alpha=0.7, label='Heart Disease', color='lightblue')\n",
        "plt.axvline(x=0.5, color='black', linestyle='--', alpha=0.7, label='Decision Threshold')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Prediction Probability Distribution', fontsize=14, fontweight='bold')\n",
        "plt.legend()\n",
        "\n",
        "# 9. Model Performance Summary\n",
        "plt.subplot(3, 3, 9)\n",
        "plt.axis('off')\n",
        "summary_text = f\"\"\"\n",
        "MODEL PERFORMANCE SUMMARY\n",
        "\n",
        "Accuracy: {accuracy:.4f}\n",
        "Precision: {precision:.4f}\n",
        "Recall: {recall:.4f}\n",
        "F1-Score: {f1:.4f}\n",
        "ROC-AUC: {roc_auc:.4f}\n",
        "\n",
        "Confusion Matrix:\n",
        "TN: {tn}    FP: {fp}\n",
        "FN: {fn}    TP: {tp}\n",
        "\n",
        "Cross-Validation:\n",
        "Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\n",
        "Precision: {cv_precision.mean():.4f} ± {cv_precision.std():.4f}\n",
        "Recall: {cv_recall.mean():.4f} ± {cv_recall.std():.4f}\n",
        "F1-Score: {cv_f1.mean():.4f} ± {cv_f1.std():.4f}\n",
        "\"\"\"\n",
        "plt.text(0.1, 0.9, summary_text, transform=plt.gca().transAxes, fontsize=10,\n",
        "         verticalalignment='top', fontfamily='monospace',\n",
        "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgray\", alpha=0.8))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
