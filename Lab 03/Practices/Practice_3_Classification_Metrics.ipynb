{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Practice 3: Classification Metrics (Accuracy, Precision, Recall, F1 Score)\n",
        "\n",
        "**Department of Electrical and Computer Engineering**  \n",
        "**Pak-Austria Fachhochschule: Institute of Applied Sciences & Technology**  \n",
        "**Subject: Machine Learning**  \n",
        "**Subject Teacher: Dr. Abid Ali**  \n",
        "**Lab Supervisor: Miss. Sana Saleem**\n",
        "\n",
        "## Objective\n",
        "Implement linear regression and evaluate it using classification metrics by converting continuous predictions to binary classifications.\n",
        "\n",
        "## Dataset\n",
        "- **File**: diabetes.csv\n",
        "- **Features**: All features except Outcome\n",
        "- **Target Variable**: Outcome (0 or 1)\n",
        "- **Threshold**: 0.5 (predictions â‰¥ 0.5 are classified as 1, < 0.5 as 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import (mean_squared_error, r2_score, \n",
        "                           accuracy_score, precision_score, \n",
        "                           recall_score, f1_score, confusion_matrix)\n",
        "from scipy import stats\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess the dataset\n",
        "url = \"diabetes.csv\"\n",
        "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
        "           'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "df = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Data preprocessing\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df = df.dropna()\n",
        "\n",
        "# Remove outliers using Z-score\n",
        "z_scores = np.abs(stats.zscore(df))\n",
        "df_clean = df[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {df_clean.shape}\")\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_clean.drop(columns='Outcome')\n",
        "y = df_clean['Outcome']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"Feature names: {list(X.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Convert continuous predictions to binary classifications\n",
        "y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]\n",
        "\n",
        "print(\"Linear Regression Model Performance:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Regression metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Metrics:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Classification metrics\n",
        "accuracy = accuracy_score(y_test, y_pred_binary)\n",
        "precision = precision_score(y_test, y_pred_binary)\n",
        "recall = recall_score(y_test, y_pred_binary)\n",
        "f1 = f1_score(y_test, y_pred_binary)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred_binary)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"True Negatives: {cm[0,0]}\")\n",
        "print(f\"False Positives: {cm[0,1]}\")\n",
        "print(f\"False Negatives: {cm[1,0]}\")\n",
        "print(f\"True Positives: {cm[1,1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visualization\n",
        "plt.figure(figsize=(18, 12))\n",
        "\n",
        "# Plot 1: Actual vs Predicted (Continuous)\n",
        "plt.subplot(3, 4, 1)\n",
        "plt.scatter(y_test, y_pred, color='purple', alpha=0.6)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)\n",
        "plt.xlabel(\"Actual Outcome\")\n",
        "plt.ylabel(\"Predicted Outcome\")\n",
        "plt.title(\"Linear Regression: Actual vs Predicted\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Residuals distribution\n",
        "plt.subplot(3, 4, 2)\n",
        "residuals = y_test - y_pred\n",
        "sns.histplot(residuals, bins=20, kde=True, color='orange')\n",
        "plt.title('Residuals Distribution')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Confusion Matrix Heatmap\n",
        "plt.subplot(3, 4, 3)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "\n",
        "# Plot 4: Classification Report Visualization\n",
        "plt.subplot(3, 4, 4)\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "values = [accuracy, precision, recall, f1]\n",
        "colors = ['skyblue', 'lightgreen', 'lightcoral', 'gold']\n",
        "bars = plt.bar(metrics, values, color=colors, alpha=0.7)\n",
        "plt.title('Classification Metrics')\n",
        "plt.ylabel('Score')\n",
        "plt.ylim(0, 1)\n",
        "for bar, value in zip(bars, values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
        "             f'{value:.3f}', ha='center', va='bottom')\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Plot 5: Prediction Distribution\n",
        "plt.subplot(3, 4, 5)\n",
        "plt.hist(y_pred, bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
        "plt.xlabel('Predicted Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Predicted Probabilities')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: ROC Curve (simplified)\n",
        "plt.subplot(3, 4, 6)\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 7: Precision-Recall Curve\n",
        "plt.subplot(3, 4, 7)\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred)\n",
        "plt.plot(recall_curve, precision_curve, color='blue', lw=2)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 8: Threshold Analysis\n",
        "plt.subplot(3, 4, 8)\n",
        "thresholds = np.linspace(0, 1, 100)\n",
        "accuracies = []\n",
        "for threshold in thresholds:\n",
        "    y_pred_thresh = [1 if pred >= threshold else 0 for pred in y_pred]\n",
        "    acc = accuracy_score(y_test, y_pred_thresh)\n",
        "    accuracies.append(acc)\n",
        "plt.plot(thresholds, accuracies, color='green', lw=2)\n",
        "plt.axvline(x=0.5, color='red', linestyle='--', linewidth=2, label='Current Threshold')\n",
        "plt.xlabel('Threshold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs Threshold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 9: Feature Importance\n",
        "plt.subplot(3, 4, 9)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': model.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=True)\n",
        "colors = ['red' if x < 0 else 'blue' for x in feature_importance['Coefficient']]\n",
        "bars = plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors, alpha=0.7)\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Feature Importance')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 10: Prediction vs Actual (Binary)\n",
        "plt.subplot(3, 4, 10)\n",
        "plt.scatter(y_test, y_pred_binary, alpha=0.6, color='purple')\n",
        "plt.xlabel(\"Actual Outcome\")\n",
        "plt.ylabel(\"Predicted Outcome (Binary)\")\n",
        "plt.title(\"Binary Classification Results\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 11: Error Analysis\n",
        "plt.subplot(3, 4, 11)\n",
        "errors = np.abs(y_test - y_pred)\n",
        "plt.scatter(y_pred, errors, alpha=0.6, color='red')\n",
        "plt.xlabel('Predicted Value')\n",
        "plt.ylabel('Absolute Error')\n",
        "plt.title('Error Analysis')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 12: Class Distribution\n",
        "plt.subplot(3, 4, 12)\n",
        "class_counts = pd.Series(y_test).value_counts()\n",
        "plt.pie(class_counts.values, labels=['Class 0', 'Class 1'], autopct='%1.1f%%', \n",
        "        colors=['lightcoral', 'lightblue'])\n",
        "plt.title('Class Distribution in Test Set')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
