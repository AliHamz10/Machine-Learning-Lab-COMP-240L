{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Practice 2: Multi-Variable Linear Regression\n",
        "\n",
        "**Department of Electrical and Computer Engineering**  \n",
        "**Pak-Austria Fachhochschule: Institute of Applied Sciences & Technology**  \n",
        "**Subject: Machine Learning**  \n",
        "**Subject Teacher: Dr. Abid Ali**  \n",
        "**Lab Supervisor: Miss. Sana Saleem**\n",
        "\n",
        "## Objective\n",
        "Implement linear regression with multiple variables using the Diabetes dataset with feature scaling and comprehensive analysis.\n",
        "\n",
        "## Dataset\n",
        "- **File**: diabetes.csv\n",
        "- **Features**: All features except Outcome (Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age)\n",
        "- **Target Variable**: Outcome (0 or 1)\n",
        "- **Preprocessing**: Feature scaling using StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy import stats\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Diabetes dataset\n",
        "url = \"diabetes.csv\"\n",
        "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
        "           'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "df = pd.read_csv(url, names=columns)\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df = df.dropna()\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {df.shape}\")\n",
        "\n",
        "# Visualize the correlation matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "correlation_matrix = df.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, \n",
        "            square=True, fmt='.2f')\n",
        "plt.title('Correlation Matrix of All Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature distributions\n",
        "df.hist(bins=20, figsize=(15, 12), color='blue', alpha=0.7)\n",
        "plt.suptitle('Feature Distributions', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier detection and removal using Z-score\n",
        "z_scores = np.abs(stats.zscore(df))\n",
        "print(\"Outliers (Z > 3):\")\n",
        "outlier_indices = np.where(z_scores > 3)\n",
        "print(f\"Number of outliers: {len(outlier_indices[0])}\")\n",
        "\n",
        "# Remove outliers\n",
        "df_clean = df[(z_scores < 3).all(axis=1)]\n",
        "print(f\"Shape after removing outliers: {df_clean.shape}\")\n",
        "\n",
        "# Visualize distributions after outlier removal\n",
        "df_clean.hist(bins=20, figsize=(15, 12), color='green', alpha=0.7)\n",
        "plt.suptitle('Distributions After Removing Outliers', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_clean.drop(columns='Outcome')\n",
        "y = df_clean['Outcome']\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n",
        "print(f\"Feature names: {list(X.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
        "\n",
        "# Feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"\\nScaled training set shape: {X_train_scaled.shape}\")\n",
        "print(f\"Scaled test set shape: {X_test_scaled.shape}\")\n",
        "\n",
        "# Visualize scaling effect\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.boxplot(X_train.values)\n",
        "plt.title('Features Before Scaling')\n",
        "plt.ylabel('Values')\n",
        "plt.xticks(range(1, len(X.columns) + 1), X.columns, rotation=45)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.boxplot(X_train_scaled)\n",
        "plt.title('Features After Scaling')\n",
        "plt.ylabel('Scaled Values')\n",
        "plt.xticks(range(1, len(X.columns) + 1), X.columns, rotation=45)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(X_train_scaled.flatten(), bins=50, alpha=0.7, color='green')\n",
        "plt.title('Distribution of All Scaled Features')\n",
        "plt.xlabel('Scaled Values')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Model trained successfully!\")\n",
        "print(f\"Model intercept: {model.intercept_:.4f}\")\n",
        "print(f\"Number of features: {len(model.coef_)}\")\n",
        "\n",
        "# Display feature coefficients\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': model.coef_\n",
        "}).sort_values('Coefficient', key=abs, ascending=False)\n",
        "\n",
        "print(\"\\nFeature Coefficients (sorted by absolute value):\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = np.mean(np.abs(y_test - y_pred))\n",
        "\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing model performance\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Actual vs Predicted\n",
        "plt.subplot(2, 3, 1)\n",
        "plt.scatter(y_test, y_pred, color='purple', alpha=0.6)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)\n",
        "plt.xlabel(\"Actual Outcome\")\n",
        "plt.ylabel(\"Predicted Outcome\")\n",
        "plt.title(\"Linear Regression: Actual vs Predicted\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Residuals distribution\n",
        "plt.subplot(2, 3, 2)\n",
        "residuals = y_test - y_pred\n",
        "sns.histplot(residuals, bins=20, kde=True, color='orange')\n",
        "plt.title('Residuals Distribution')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Feature importance\n",
        "plt.subplot(2, 3, 3)\n",
        "feature_importance_sorted = feature_importance.sort_values('Coefficient', key=abs, ascending=True)\n",
        "colors = ['red' if x < 0 else 'blue' for x in feature_importance_sorted['Coefficient']]\n",
        "bars = plt.barh(feature_importance_sorted['Feature'], feature_importance_sorted['Coefficient'], color=colors, alpha=0.7)\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Feature Importance in Linear Regression')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Training and Loss Visualization (Epochs Simulation)\n",
        "plt.subplot(2, 3, 4)\n",
        "epochs = 500\n",
        "train_errors = []\n",
        "for i in range(epochs):\n",
        "    model_temp = LinearRegression()\n",
        "    model_temp.fit(X_train_scaled, y_train)\n",
        "    y_train_pred = model_temp.predict(X_train_scaled)\n",
        "    error = mean_squared_error(y_train, y_train_pred)\n",
        "    train_errors.append(error)\n",
        "\n",
        "plt.plot(range(epochs), train_errors, color='blue')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Error (MSE)')\n",
        "plt.title('Training Error Over Epochs')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Residuals vs Predicted\n",
        "plt.subplot(2, 3, 5)\n",
        "plt.scatter(y_pred, residuals, alpha=0.6, color='green')\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residuals vs Predicted')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Q-Q plot for residuals\n",
        "plt.subplot(2, 3, 6)\n",
        "from scipy import stats\n",
        "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
        "plt.title('Q-Q Plot of Residuals')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
