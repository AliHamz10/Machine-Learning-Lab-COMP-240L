{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Practice 4: Cost Function and Gradient Descent Implementation\n",
        "\n",
        "**Department of Electrical and Computer Engineering**  \n",
        "**Pak-Austria Fachhochschule: Institute of Applied Sciences & Technology**  \n",
        "**Subject: Machine Learning**  \n",
        "**Subject Teacher: Dr. Abid Ali**  \n",
        "**Lab Supervisor: Miss. Sana Saleem**\n",
        "\n",
        "## Objective\n",
        "Implement gradient descent algorithm from scratch to optimize linear regression parameters and visualize the cost function over iterations.\n",
        "\n",
        "## Dataset\n",
        "- **File**: diabetes.csv\n",
        "- **Features**: All features except Outcome\n",
        "- **Target Variable**: Outcome (0 or 1)\n",
        "- **Algorithm**: Custom gradient descent implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preprocess the dataset\n",
        "url = \"diabetes.csv\"\n",
        "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', \n",
        "           'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
        "df = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Data preprocessing\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df = df.dropna()\n",
        "\n",
        "# Remove outliers using Z-score\n",
        "z_scores = np.abs(stats.zscore(df))\n",
        "df_clean = df[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "print(f\"Dataset shape after cleaning: {df_clean.shape}\")\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_clean.drop(columns='Outcome')\n",
        "y = df_clean['Outcome']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set size: {X_train_scaled.shape[0]}\")\n",
        "print(f\"Test set size: {X_test_scaled.shape[0]}\")\n",
        "print(f\"Number of features: {X_train_scaled.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize parameters\n",
        "np.random.seed(42)\n",
        "m, n = X_train_scaled.shape\n",
        "theta = np.random.randn(n)  # Initial weights\n",
        "bias = 0.0  # Initial bias\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "print(f\"Initial parameters:\")\n",
        "print(f\"Theta shape: {theta.shape}\")\n",
        "print(f\"Initial theta: {theta}\")\n",
        "print(f\"Initial bias: {bias}\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Number of epochs: {epochs}\")\n",
        "\n",
        "# Define the cost function (MSE)\n",
        "def compute_cost(X, y, theta, bias):\n",
        "    \"\"\"\n",
        "    Compute the mean squared error cost function\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    predictions = np.dot(X, theta) + bias\n",
        "    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
        "    return cost\n",
        "\n",
        "# Define gradient descent function\n",
        "def gradient_descent(X, y, theta, bias, learning_rate, epochs):\n",
        "    \"\"\"\n",
        "    Perform gradient descent optimization\n",
        "    \"\"\"\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Make predictions\n",
        "        predictions = np.dot(X, theta) + bias\n",
        "        \n",
        "        # Compute gradients\n",
        "        d_theta = (1 / m) * np.dot(X.T, (predictions - y))\n",
        "        d_bias = (1 / m) * np.sum(predictions - y)\n",
        "        \n",
        "        # Update weights\n",
        "        theta -= learning_rate * d_theta\n",
        "        bias -= learning_rate * d_bias\n",
        "        \n",
        "        # Calculate cost and save it for plotting\n",
        "        cost = compute_cost(X, y, theta, bias)\n",
        "        cost_history.append(cost)\n",
        "        \n",
        "        # Print progress every 100 epochs\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Cost = {cost:.6f}\")\n",
        "    \n",
        "    return theta, bias, cost_history\n",
        "\n",
        "print(\"\\nStarting Gradient Descent Optimization...\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run gradient descent\n",
        "theta, bias, cost_history = gradient_descent(X_train_scaled, y_train, theta, bias, learning_rate, epochs)\n",
        "\n",
        "print(f\"\\nGradient Descent Completed!\")\n",
        "print(f\"Final theta: {theta}\")\n",
        "print(f\"Final bias: {bias:.6f}\")\n",
        "print(f\"Final cost: {cost_history[-1]:.6f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred_train = np.dot(X_train_scaled, theta) + bias\n",
        "y_pred_test = np.dot(X_test_scaled, theta) + bias\n",
        "\n",
        "# Calculate test MSE\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "print(f\"Test MSE: {mse_test:.6f}\")\n",
        "\n",
        "# Calculate R-squared\n",
        "def r2_score_custom(y_true, y_pred):\n",
        "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
        "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "r2_test = r2_score_custom(y_test, y_pred_test)\n",
        "print(f\"Test RÂ²: {r2_test:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visualization\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Plot 1: Cost function over epochs\n",
        "plt.subplot(3, 4, 1)\n",
        "plt.plot(range(epochs), cost_history, color='blue', linewidth=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.title('Cost Function Over Epochs (Gradient Descent)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Cost function (log scale)\n",
        "plt.subplot(3, 4, 2)\n",
        "plt.plot(range(epochs), cost_history, color='red', linewidth=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost (MSE) - Log Scale')\n",
        "plt.title('Cost Function (Log Scale)')\n",
        "plt.yscale('log')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Actual vs Predicted (Training)\n",
        "plt.subplot(3, 4, 3)\n",
        "plt.scatter(y_train, y_pred_train, color='blue', alpha=0.6)\n",
        "plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', linewidth=2)\n",
        "plt.xlabel('Actual Outcome')\n",
        "plt.ylabel('Predicted Outcome')\n",
        "plt.title('Training: Actual vs Predicted')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Actual vs Predicted (Test)\n",
        "plt.subplot(3, 4, 4)\n",
        "plt.scatter(y_test, y_pred_test, color='green', alpha=0.6)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linewidth=2)\n",
        "plt.xlabel('Actual Outcome')\n",
        "plt.ylabel('Predicted Outcome')\n",
        "plt.title('Test: Actual vs Predicted')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 5: Residuals (Training)\n",
        "plt.subplot(3, 4, 5)\n",
        "train_residuals = y_train - y_pred_train\n",
        "plt.scatter(y_pred_train, train_residuals, color='blue', alpha=0.6)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Training Residuals')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 6: Residuals (Test)\n",
        "plt.subplot(3, 4, 6)\n",
        "test_residuals = y_test - y_pred_test\n",
        "plt.scatter(y_pred_test, test_residuals, color='green', alpha=0.6)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Test Residuals')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 7: Feature importance (coefficients)\n",
        "plt.subplot(3, 4, 7)\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': theta\n",
        "}).sort_values('Coefficient', key=abs, ascending=True)\n",
        "colors = ['red' if x < 0 else 'blue' for x in feature_importance['Coefficient']]\n",
        "bars = plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color=colors, alpha=0.7)\n",
        "plt.xlabel('Coefficient Value')\n",
        "plt.title('Feature Importance After Gradient Descent')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 8: Learning curve (cost vs iterations)\n",
        "plt.subplot(3, 4, 8)\n",
        "plt.plot(range(100, epochs), cost_history[100:], color='purple', linewidth=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.title('Learning Curve (Epochs 100-1000)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 9: Cost convergence\n",
        "plt.subplot(3, 4, 9)\n",
        "cost_diff = np.diff(cost_history)\n",
        "plt.plot(range(1, len(cost_diff)+1), cost_diff, color='orange', linewidth=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost Change')\n",
        "plt.title('Cost Convergence Rate')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 10: Prediction distribution\n",
        "plt.subplot(3, 4, 10)\n",
        "plt.hist(y_pred_test, bins=30, alpha=0.7, color='lightblue', edgecolor='black')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Test Predictions')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 11: Error distribution\n",
        "plt.subplot(3, 4, 11)\n",
        "plt.hist(test_residuals, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Test Residuals')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 12: Comparison with sklearn\n",
        "plt.subplot(3, 4, 12)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "sklearn_model = LinearRegression()\n",
        "sklearn_model.fit(X_train_scaled, y_train)\n",
        "sklearn_pred = sklearn_model.predict(X_test_scaled)\n",
        "\n",
        "plt.scatter(y_test, y_pred_test, color='blue', alpha=0.6, label='Gradient Descent')\n",
        "plt.scatter(y_test, sklearn_pred, color='red', alpha=0.6, label='Sklearn')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='black', linewidth=2)\n",
        "plt.xlabel('Actual Outcome')\n",
        "plt.ylabel('Predicted Outcome')\n",
        "plt.title('Gradient Descent vs Sklearn')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare with sklearn results\n",
        "sklearn_mse = mean_squared_error(y_test, sklearn_pred)\n",
        "sklearn_r2 = r2_score_custom(y_test, sklearn_pred)\n",
        "\n",
        "print(f\"\\nComparison with Sklearn:\")\n",
        "print(f\"Gradient Descent - MSE: {mse_test:.6f}, RÂ²: {r2_test:.6f}\")\n",
        "print(f\"Sklearn - MSE: {sklearn_mse:.6f}, RÂ²: {sklearn_r2:.6f}\")\n",
        "print(f\"Difference - MSE: {abs(mse_test - sklearn_mse):.6f}, RÂ²: {abs(r2_test - sklearn_r2):.6f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
