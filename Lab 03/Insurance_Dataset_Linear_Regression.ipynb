{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 03: Linear Regression on Insurance Dataset\n",
        "\n",
        "**Subject:** Machine Learning  \n",
        "**Subject Teacher:** Dr. Abid Ali  \n",
        "**Lab Supervisor:** Miss. Sana Saleem  \n",
        "**Student:** [Your Name]\n",
        "\n",
        "## Objectives\n",
        "- Implement linear regression to predict insurance charges using real-world dataset\n",
        "- Explore and preprocess datasets, including handling missing values and outliers\n",
        "- Apply data visualization techniques to understand data distributions and correlations\n",
        "- Train and evaluate a linear regression model using performance metrics\n",
        "- Use gradient descent to optimize model parameters and understand the cost function\n",
        "- Analyze model performance using metrics such as accuracy, precision, recall, and F1-score\n",
        "- Visualize the importance of features in the model and analyze training loss over time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 1: Data Analysis and Preprocessing\n",
        "\n",
        "### 1.1 Import Libraries and Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Insurance dataset\n",
        "df = pd.read_csv('insurance[1].csv')\n",
        "\n",
        "# Display first few rows of the dataset\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Data Quality Assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Check for duplicates\n",
        "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Check unique values in categorical columns\n",
        "print(\"\\nUnique values in categorical columns:\")\n",
        "categorical_cols = ['gender', 'smoker', 'region']\n",
        "for col in categorical_cols:\n",
        "    print(f\"{col}: {df[col].unique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.3 Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle categorical variables using Label Encoding\n",
        "le_gender = LabelEncoder()\n",
        "le_smoker = LabelEncoder()\n",
        "le_region = LabelEncoder()\n",
        "\n",
        "df['gender_encoded'] = le_gender.fit_transform(df['gender'])\n",
        "df['smoker_encoded'] = le_smoker.fit_transform(df['smoker'])\n",
        "df['region_encoded'] = le_region.fit_transform(df['region'])\n",
        "\n",
        "print(\"Encoded values:\")\n",
        "print(f\"Gender: {dict(zip(le_gender.classes_, le_gender.transform(le_gender.classes_)))}\")\n",
        "print(f\"Smoker: {dict(zip(le_smoker.classes_, le_smoker.transform(le_smoker.classes_)))}\")\n",
        "print(f\"Region: {dict(zip(le_region.classes_, le_region.transform(le_region.classes_)))}\")\n",
        "\n",
        "# Create feature matrix with encoded variables\n",
        "feature_cols = ['age', 'gender_encoded', 'bmi', 'children', 'smoker_encoded', 'region_encoded']\n",
        "X = df[feature_cols]\n",
        "y = df['charges']\n",
        "\n",
        "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
        "print(f\"Target variable shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Outlier Analysis using Z-score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Outlier analysis using Z-score\n",
        "z_scores = np.abs(stats.zscore(X))\n",
        "print(\"Outliers (Z > 3):\")\n",
        "outlier_indices = np.where(z_scores > 3)\n",
        "print(f\"Number of outliers: {len(outlier_indices[0])}\")\n",
        "\n",
        "# Remove outliers based on Z-score\n",
        "df_clean = df[(z_scores < 3).all(axis=1)]\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "print(f\"Shape after removing outliers: {df_clean.shape}\")\n",
        "print(f\"Percentage of data retained: {(df_clean.shape[0]/df.shape[0])*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 2: Data Visualization\n",
        "\n",
        "### 2.1 Correlation Matrix and Feature Distributions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare clean data for visualization\n",
        "X_clean = df_clean[feature_cols]\n",
        "y_clean = df_clean['charges']\n",
        "\n",
        "# Visualize the correlation matrix\n",
        "plt.figure(figsize=(12, 8))\n",
        "correlation_data = X_clean.copy()\n",
        "correlation_data['charges'] = y_clean\n",
        "sns.heatmap(correlation_data.corr(), annot=True, cmap='coolwarm', linewidths=0.5, fmt='.3f')\n",
        "plt.title('Correlation Matrix - Insurance Dataset')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature distributions before and after outlier removal\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "# Before outlier removal\n",
        "df[feature_cols + ['charges']].hist(bins=20, ax=axes[0], color='blue', alpha=0.7)\n",
        "axes[0].set_title('Feature Distributions - Before Outlier Removal')\n",
        "\n",
        "# After outlier removal\n",
        "df_clean[feature_cols + ['charges']].hist(bins=20, ax=axes[1], color='green', alpha=0.7)\n",
        "axes[1].set_title('Feature Distributions - After Outlier Removal')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 3: Linear Regression Implementation\n",
        "\n",
        "### Lab Practice 1: Single Variable Linear Regression (Age vs Charges)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single variable linear regression using Age as feature\n",
        "X_single = X_clean[['age']]  # Single input feature\n",
        "y_single = y_clean  # Target variable\n",
        "\n",
        "# Train-test split\n",
        "X_train_single, X_test_single, y_train_single, y_test_single = train_test_split(\n",
        "    X_single, y_single, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train the Linear Regression model\n",
        "model_single = LinearRegression()\n",
        "model_single.fit(X_train_single, y_train_single)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_single = model_single.predict(X_test_single)\n",
        "\n",
        "# Calculate metrics\n",
        "mse_single = mean_squared_error(y_test_single, y_pred_single)\n",
        "r2_single = r2_score(y_test_single, y_pred_single)\n",
        "\n",
        "print(f\"Single Variable Linear Regression (Age vs Charges):\")\n",
        "print(f\"Mean Squared Error: {mse_single:.2f}\")\n",
        "print(f\"R-squared: {r2_single:.4f}\")\n",
        "print(f\"Coefficient (Age): {model_single.coef_[0]:.2f}\")\n",
        "print(f\"Intercept: {model_single.intercept_:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing single variable model performance\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Actual vs Predicted\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test_single, y_pred_single, color='purple', alpha=0.6)\n",
        "plt.plot([y_test_single.min(), y_test_single.max()], \n",
        "         [y_test_single.min(), y_test_single.max()], \n",
        "         color='red', linewidth=2, linestyle='--')\n",
        "plt.xlabel(\"Actual Charges\")\n",
        "plt.ylabel(\"Predicted Charges\")\n",
        "plt.title(\"Single Variable LR: Actual vs Predicted\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals_single = y_test_single - y_pred_single\n",
        "plt.scatter(y_pred_single, residuals_single, color='orange', alpha=0.6)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel(\"Predicted Charges\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals Plot\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lab Practice 2: Multi-Variable Linear Regression with Feature Scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-variable linear regression\n",
        "X_multi = X_clean  # All features\n",
        "y_multi = y_clean  # Target variable\n",
        "\n",
        "# Train-test split\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
        "    X_multi, y_multi, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Feature scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_multi)\n",
        "X_test_scaled = scaler.transform(X_test_multi)\n",
        "\n",
        "# Train the model\n",
        "model_multi = LinearRegression()\n",
        "model_multi.fit(X_train_scaled, y_train_multi)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred_multi = model_multi.predict(X_test_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "mse_multi = mean_squared_error(y_test_multi, y_pred_multi)\n",
        "r2_multi = r2_score(y_test_multi, y_pred_multi)\n",
        "\n",
        "print(f\"Multi-Variable Linear Regression:\")\n",
        "print(f\"Mean Squared Error: {mse_multi:.2f}\")\n",
        "print(f\"R-squared: {r2_multi:.4f}\")\n",
        "print(f\"\\nFeature Coefficients:\")\n",
        "for feature, coef in zip(feature_cols, model_multi.coef_):\n",
        "    print(f\"{feature}: {coef:.4f}\")\n",
        "print(f\"Intercept: {model_multi.intercept_:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing multi-variable model performance\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Actual vs Predicted\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_test_multi, y_pred_multi, color='purple', alpha=0.6)\n",
        "plt.plot([y_test_multi.min(), y_test_multi.max()], \n",
        "         [y_test_multi.min(), y_test_multi.max()], \n",
        "         color='red', linewidth=2, linestyle='--')\n",
        "plt.xlabel(\"Actual Charges\")\n",
        "plt.ylabel(\"Predicted Charges\")\n",
        "plt.title(\"Multi-Variable LR: Actual vs Predicted\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Residuals\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals_multi = y_test_multi - y_pred_multi\n",
        "plt.scatter(y_pred_multi, residuals_multi, color='orange', alpha=0.6)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel(\"Predicted Charges\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals Plot\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lab Practice 3: Performance Metrics (Accuracy, Precision, Recall, F1-Score)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For classification metrics, we need to convert regression predictions to binary\n",
        "# Let's create a binary classification problem: high charges vs low charges\n",
        "charges_median = y_clean.median()\n",
        "y_binary = (y_clean > charges_median).astype(int)\n",
        "\n",
        "print(f\"Median charges: ${charges_median:.2f}\")\n",
        "print(f\"High charges (>median): {y_binary.sum()} samples\")\n",
        "print(f\"Low charges (<=median): {(y_binary == 0).sum()} samples\")\n",
        "\n",
        "# Split for binary classification\n",
        "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(\n",
        "    X_clean, y_binary, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler_binary = StandardScaler()\n",
        "X_train_binary_scaled = scaler_binary.fit_transform(X_train_binary)\n",
        "X_test_binary_scaled = scaler_binary.transform(X_test_binary)\n",
        "\n",
        "# Train model\n",
        "model_binary = LinearRegression()\n",
        "model_binary.fit(X_train_binary_scaled, y_train_binary)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_proba = model_binary.predict(X_test_binary_scaled)\n",
        "y_pred_binary = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "# Calculate all metrics\n",
        "accuracy = accuracy_score(y_test_binary, y_pred_binary)\n",
        "precision = precision_score(y_test_binary, y_pred_binary)\n",
        "recall = recall_score(y_test_binary, y_pred_binary)\n",
        "f1 = f1_score(y_test_binary, y_pred_binary)\n",
        "mse_binary = mean_squared_error(y_test_binary, y_pred_proba)\n",
        "r2_binary = r2_score(y_test_binary, y_pred_proba)\n",
        "\n",
        "print(f\"\\nBinary Classification Metrics (High vs Low Charges):\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Mean Squared Error: {mse_binary:.4f}\")\n",
        "print(f\"R-squared: {r2_binary:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Lab Practice 4: Custom Gradient Descent and Cost Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom Gradient Descent Implementation\n",
        "def compute_cost(X, y, theta, bias):\n",
        "    \"\"\"Compute the cost function (MSE)\"\"\"\n",
        "    m = len(y)\n",
        "    predictions = np.dot(X, theta) + bias\n",
        "    cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)\n",
        "    return cost\n",
        "\n",
        "def gradient_descent(X, y, theta, bias, learning_rate, epochs):\n",
        "    \"\"\"Implement gradient descent\"\"\"\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        # Make predictions\n",
        "        predictions = np.dot(X, theta) + bias\n",
        "        \n",
        "        # Compute gradients\n",
        "        d_theta = (1 / m) * np.dot(X.T, (predictions - y))\n",
        "        d_bias = (1 / m) * np.sum(predictions - y)\n",
        "        \n",
        "        # Update weights\n",
        "        theta -= learning_rate * d_theta\n",
        "        bias -= learning_rate * d_bias\n",
        "        \n",
        "        # Calculate cost and save it for plotting\n",
        "        cost = compute_cost(X, y, theta, bias)\n",
        "        cost_history.append(cost)\n",
        "        \n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Cost = {cost:.4f}\")\n",
        "    \n",
        "    return theta, bias, cost_history\n",
        "\n",
        "# Initialize parameters\n",
        "np.random.seed(42)\n",
        "m, n = X_train_scaled.shape\n",
        "theta = np.random.randn(n)  # Initial weights\n",
        "bias = 0.0  # Initial bias\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "print(f\"Starting Gradient Descent with {epochs} epochs...\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "print(f\"Initial cost: {compute_cost(X_train_scaled, y_train_multi, theta, bias):.4f}\")\n",
        "print(\"\\nTraining progress:\")\n",
        "\n",
        "# Run gradient descent\n",
        "theta_optimized, bias_optimized, cost_history = gradient_descent(\n",
        "    X_train_scaled, y_train_multi, theta, bias, learning_rate, epochs\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal cost: {cost_history[-1]:.4f}\")\n",
        "print(f\"Final weights: {theta_optimized}\")\n",
        "print(f\"Final bias: {bias_optimized:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions using custom gradient descent\n",
        "y_pred_train_gd = np.dot(X_train_scaled, theta_optimized) + bias_optimized\n",
        "y_pred_test_gd = np.dot(X_test_scaled, theta_optimized) + bias_optimized\n",
        "\n",
        "# Calculate metrics\n",
        "mse_test_gd = mean_squared_error(y_test_multi, y_pred_test_gd)\n",
        "r2_test_gd = r2_score(y_test_multi, y_pred_test_gd)\n",
        "\n",
        "print(f\"Custom Gradient Descent Results:\")\n",
        "print(f\"Test MSE: {mse_test_gd:.2f}\")\n",
        "print(f\"Test R-squared: {r2_test_gd:.4f}\")\n",
        "\n",
        "# Plot cost over epochs\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(epochs), cost_history, color='blue', linewidth=2)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost (MSE)')\n",
        "plt.title('Cost Over Epochs (Gradient Descent)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_test_multi, y_pred_test_gd, color='purple', alpha=0.6)\n",
        "plt.plot([y_test_multi.min(), y_test_multi.max()], \n",
        "         [y_test_multi.min(), y_test_multi.max()], \n",
        "         color='red', linewidth=2, linestyle='--')\n",
        "plt.xlabel(\"Actual Charges\")\n",
        "plt.ylabel(\"Predicted Charges\")\n",
        "plt.title(\"Gradient Descent: Actual vs Predicted\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Phase 4: Results Summary and Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of all models\n",
        "print(\"=\" * 60)\n",
        "print(\"LINEAR REGRESSION MODELS COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n1. Single Variable (Age only):\")\n",
        "print(f\"   MSE: {mse_single:.2f}\")\n",
        "print(f\"   R²:  {r2_single:.4f}\")\n",
        "\n",
        "print(f\"\\n2. Multi-Variable (All features):\")\n",
        "print(f\"   MSE: {mse_multi:.2f}\")\n",
        "print(f\"   R²:  {r2_multi:.4f}\")\n",
        "\n",
        "print(f\"\\n3. Custom Gradient Descent:\")\n",
        "print(f\"   MSE: {mse_test_gd:.2f}\")\n",
        "print(f\"   R²:  {r2_test_gd:.4f}\")\n",
        "\n",
        "print(f\"\\n4. Binary Classification (High vs Low Charges):\")\n",
        "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"   Precision: {precision:.4f}\")\n",
        "print(f\"   Recall:    {recall:.4f}\")\n",
        "print(f\"   F1-Score:  {f1:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"KEY INSIGHTS:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"• Dataset contains {df.shape[0]} samples with {df.shape[1]} features\")\n",
        "print(f\"• After outlier removal: {df_clean.shape[0]} samples ({df_clean.shape[0]/df.shape[0]*100:.1f}% retained)\")\n",
        "print(f\"• Most important features: {feature_cols[np.argmax(np.abs(model_multi.coef_))]} (coef: {model_multi.coef_[np.argmax(np.abs(model_multi.coef_))]:.4f})\")\n",
        "print(f\"• Multi-variable model performs {'better' if r2_multi > r2_single else 'worse'} than single variable model\")\n",
        "print(f\"• Custom gradient descent {'matches' if abs(r2_test_gd - r2_multi) < 0.01 else 'differs from'} sklearn implementation\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
