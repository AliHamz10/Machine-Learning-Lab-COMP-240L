{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CCP: Comparative Machine Learning Framework for Heart Disease Prediction\n",
        "\n",
        "**Course:** COMP-240L Machine Learning Lab  \n",
        "**Students:** Ali Hamza & Zarmeena Jawad  \n",
        "**Student Numbers:** B23F0063AI106 & B23F0115AI125  \n",
        "**Dataset:** Cleveland Heart Disease Dataset  \n",
        "**Objective:** Develop and critically evaluate a predictive diagnostic system using multiple machine learning algorithms\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project implements a comprehensive comparative machine learning framework to predict the presence or absence of heart disease using patient clinical data. The framework evaluates five different algorithms:\n",
        "\n",
        "1. **Decision Tree (DT)**\n",
        "2. **Random Forest (RF)**\n",
        "3. **Support Vector Machine (SVM)**\n",
        "4. **Artificial Neural Network (ANN)**\n",
        "5. **K-Nearest Neighbors (KNN)**\n",
        "\n",
        "The goal is to determine which model offers the best trade-off between accuracy, precision, and interpretability for medical diagnostic prediction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup\n",
        "\n",
        "We'll import all necessary libraries for data processing, machine learning, and visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n",
        "                           precision_score, recall_score, f1_score, roc_auc_score,\n",
        "                           roc_curve)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ“ All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column names for Cleveland Heart Disease dataset\n",
        "column_names = [\n",
        "    'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
        "    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'\n",
        "]\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(url, names=column_names, na_values='?')\n",
        "    print(\"âœ“ Dataset loaded successfully from UCI ML Repository\")\n",
        "except:\n",
        "    # Try alternative source\n",
        "    try:\n",
        "        df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/heart.csv\")\n",
        "        print(\"âœ“ Dataset loaded from alternative source\")\n",
        "    except:\n",
        "        # Load from local file if available\n",
        "        df = pd.read_csv('../Data/heart_disease.csv', names=column_names, na_values='?')\n",
        "        print(\"âœ“ Dataset loaded from local file\")\n",
        "\n",
        "# Convert target to binary (0 = no disease, 1-4 = disease)\n",
        "df['target'] = (df['target'] > 0).astype(int)\n",
        "\n",
        "print(f\"\\nDataset Shape: {df.shape}\")\n",
        "print(f\"Features: {df.shape[1]-1}\")\n",
        "print(f\"Instances: {df.shape[0]}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis (EDA)\n",
        "\n",
        "Let's explore the dataset to understand its characteristics, distributions, and relationships between features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information\n",
        "print(\"Dataset Info:\")\n",
        "print(df.info())\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Statistical Summary:\")\n",
        "print(df.describe())\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Target Distribution:\")\n",
        "print(df['target'].value_counts())\n",
        "print(f\"\\nClass 0 (No Disease): {df['target'].value_counts()[0]} ({df['target'].value_counts()[0]/len(df)*100:.2f}%)\")\n",
        "print(f\"Class 1 (Disease): {df['target'].value_counts()[1]} ({df['target'].value_counts()[1]/len(df)*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive EDA visualizations\n",
        "fig = plt.figure(figsize=(20, 14))\n",
        "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "fig.suptitle('Heart Disease Dataset - Exploratory Data Analysis', \n",
        "             fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "# Target distribution\n",
        "ax1 = fig.add_subplot(gs[0, 0])\n",
        "target_counts = df['target'].value_counts()\n",
        "colors = ['#ff9999', '#66b3ff']\n",
        "ax1.bar(['No Disease', 'Disease'], target_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
        "ax1.set_title('Target Variable Distribution', fontweight='bold')\n",
        "ax1.set_ylabel('Count')\n",
        "for i, v in enumerate(target_counts.values):\n",
        "    ax1.text(i, v + 5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# Correlation heatmap\n",
        "ax2 = fig.add_subplot(gs[0, 1:])\n",
        "corr_matrix = df.corr()\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "           ax=ax2, cbar_kws={'shrink': 0.8}, fmt='.2f', square=True)\n",
        "ax2.set_title('Feature Correlation Matrix', fontweight='bold')\n",
        "\n",
        "# Age distribution by target\n",
        "ax3 = fig.add_subplot(gs[1, 0])\n",
        "df.boxplot(column='age', by='target', ax=ax3)\n",
        "ax3.set_title('Age Distribution by Disease Status', fontweight='bold')\n",
        "ax3.set_xlabel('Disease Status')\n",
        "ax3.set_ylabel('Age')\n",
        "plt.setp(ax3.get_xticklabels(), rotation=0)\n",
        "\n",
        "# Cholesterol distribution by target\n",
        "ax4 = fig.add_subplot(gs[1, 1])\n",
        "df.boxplot(column='chol', by='target', ax=ax4)\n",
        "ax4.set_title('Cholesterol Distribution by Disease Status', fontweight='bold')\n",
        "ax4.set_xlabel('Disease Status')\n",
        "ax4.set_ylabel('Cholesterol (mg/dl)')\n",
        "plt.setp(ax4.get_xticklabels(), rotation=0)\n",
        "\n",
        "# Maximum heart rate by target\n",
        "ax5 = fig.add_subplot(gs[1, 2])\n",
        "df.boxplot(column='thalach', by='target', ax=ax5)\n",
        "ax5.set_title('Max Heart Rate by Disease Status', fontweight='bold')\n",
        "ax5.set_xlabel('Disease Status')\n",
        "ax5.set_ylabel('Max Heart Rate')\n",
        "plt.setp(ax5.get_xticklabels(), rotation=0)\n",
        "\n",
        "# Feature correlation with target\n",
        "ax6 = fig.add_subplot(gs[2, 0])\n",
        "target_corr = corr_matrix['target'].drop('target').abs().sort_values(ascending=True)\n",
        "target_corr.plot(kind='barh', ax=ax6, color='lightcoral')\n",
        "ax6.set_title('Feature Correlation with Target', fontweight='bold')\n",
        "ax6.set_xlabel('Absolute Correlation')\n",
        "\n",
        "# Sex distribution by target\n",
        "ax7 = fig.add_subplot(gs[2, 1])\n",
        "sex_target = pd.crosstab(df['sex'], df['target'])\n",
        "sex_target.plot(kind='bar', ax=ax7, color=['#ff9999', '#66b3ff'], alpha=0.7)\n",
        "ax7.set_title('Disease Status by Sex', fontweight='bold')\n",
        "ax7.set_xlabel('Sex (0=Female, 1=Male)')\n",
        "ax7.set_ylabel('Count')\n",
        "ax7.legend(['No Disease', 'Disease'])\n",
        "ax7.set_xticklabels(['Female', 'Male'], rotation=0)\n",
        "\n",
        "# Chest pain type by target\n",
        "ax8 = fig.add_subplot(gs[2, 2])\n",
        "cp_target = pd.crosstab(df['cp'], df['target'])\n",
        "cp_target.plot(kind='bar', ax=ax8, color=['#ff9999', '#66b3ff'], alpha=0.7)\n",
        "ax8.set_title('Disease Status by Chest Pain Type', fontweight='bold')\n",
        "ax8.set_xlabel('Chest Pain Type')\n",
        "ax8.set_ylabel('Count')\n",
        "ax8.legend(['No Disease', 'Disease'])\n",
        "ax8.set_xticklabels(['Typical Angina', 'Atypical Angina', \n",
        "                    'Non-anginal Pain', 'Asymptomatic'], rotation=45, ha='right')\n",
        "\n",
        "plt.savefig('../Results/heart_disease_eda.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"âœ“ EDA visualizations saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Preprocessing\n",
        "\n",
        "We'll handle missing values, perform feature scaling, and split the data into training and testing sets. We'll also address class imbalance using SMOTE if necessary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "df_processed = df.copy()\n",
        "numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    if df_processed[col].isnull().sum() > 0:\n",
        "        median_val = df_processed[col].median()\n",
        "        df_processed[col].fillna(median_val, inplace=True)\n",
        "        print(f\"âœ“ Filled {col} missing values with median: {median_val:.2f}\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df_processed.drop('target', axis=1)\n",
        "y = df_processed['target']\n",
        "\n",
        "print(f\"\\nFeatures shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Train-test split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining class distribution:\\n{y_train.value_counts().to_dict()}\")\n",
        "print(f\"Test class distribution:\\n{y_test.value_counts().to_dict()}\")\n",
        "\n",
        "# Handle class imbalance using SMOTE (before scaling to ensure consistency)\n",
        "imbalance_ratio = min(y_train.value_counts()) / max(y_train.value_counts())\n",
        "print(f\"\\nClass imbalance ratio: {imbalance_ratio:.3f}\")\n",
        "\n",
        "if imbalance_ratio < 0.6:\n",
        "    print(\"Applying SMOTE to balance training data...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    # Apply SMOTE to unscaled data first to ensure both scaled and unscaled versions match\n",
        "    X_train, y_train = smote.fit_resample(X_train, y_train)\n",
        "    print(f\"âœ“ Training data after SMOTE: {X_train.shape[0]} samples\")\n",
        "    print(f\"Balanced class distribution:\\n{pd.Series(y_train).value_counts().to_dict()}\")\n",
        "else:\n",
        "    print(\"Class imbalance is acceptable, skipping SMOTE\")\n",
        "\n",
        "# Feature scaling (after SMOTE to ensure scaled data matches resampled data)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"\\nâœ“ Features scaled using StandardScaler\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Learning Curves\n",
        "\n",
        "Learning curves help us understand if a model is overfitting or underfitting by showing how the model's performance changes as the training set size increases.\n",
        "\n",
        "### Learning Curve for Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: Learning curve code has been moved to Section 9 (after Model Evaluation)\n",
        "# This ensures comparison_df and trained_models are available before execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n",
        "\n",
        "We'll train five different machine learning models:\n",
        "1. Decision Tree\n",
        "2. Random Forest\n",
        "3. Support Vector Machine (SVM)\n",
        "4. Artificial Neural Network (ANN)\n",
        "5. K-Nearest Neighbors (KNN)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "    'SVM': SVC(random_state=42, probability=True, kernel='rbf'),\n",
        "    'ANN': MLPClassifier(random_state=42, hidden_layer_sizes=(100, 50), \n",
        "                        max_iter=500, early_stopping=True),\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "# Train each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Select appropriate data (scaled for SVM, ANN, KNN)\n",
        "    if name in ['SVM', 'ANN', 'KNN']:\n",
        "        X_train_use = X_train_scaled\n",
        "        X_test_use = X_test_scaled\n",
        "    else:\n",
        "        X_train_use = X_train.values\n",
        "        X_test_use = X_test.values\n",
        "    \n",
        "    # Train model\n",
        "    model.fit(X_train_use, y_train)\n",
        "    print(f\"âœ“ {name} trained successfully\")\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_use)\n",
        "    y_pred_proba = model.predict_proba(X_test_use)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "    \n",
        "    # AUC-ROC\n",
        "    auc_roc = None\n",
        "    if y_pred_proba is not None:\n",
        "        try:\n",
        "            auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Cross-validation score\n",
        "    cv_scores = cross_val_score(model, X_train_use, y_train, cv=5, scoring='accuracy')\n",
        "    cv_mean = cv_scores.mean()\n",
        "    cv_std = cv_scores.std()\n",
        "    \n",
        "    # Store results\n",
        "    trained_models[name] = model\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'auc_roc': auc_roc,\n",
        "        'cv_mean': cv_mean,\n",
        "        'cv_std': cv_std,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "    if auc_roc:\n",
        "        print(f\"  AUC-ROC: {auc_roc:.4f}\")\n",
        "    print(f\"  CV Score (mean Â± std): {cv_mean:.4f} Â± {cv_std:.4f}\")\n",
        "\n",
        "print(\"\\nâœ“ All models trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Conclusion and Recommendations\n",
        "\n",
        "Based on the comprehensive evaluation, we can now make data-driven recommendations for the best model for heart disease prediction in this context.\n",
        "\n",
        "### Key Findings:\n",
        "1. All models were successfully trained and evaluated\n",
        "2. Hyperparameter tuning improved performance for Random Forest and SVM\n",
        "3. The best model achieved high accuracy and F1-score\n",
        "4. Learning curves help identify overfitting/underfitting issues\n",
        "\n",
        "### Model Selection Justification:\n",
        "The best model selection should consider:\n",
        "- **Accuracy**: Overall correctness of predictions\n",
        "- **Precision**: Minimizing false positives (critical in medical diagnosis)\n",
        "- **Recall**: Minimizing false negatives (missing actual disease cases)\n",
        "- **F1-Score**: Balanced measure of precision and recall\n",
        "- **AUC-ROC**: Overall discriminative ability\n",
        "- **Interpretability**: Ability to explain predictions (important for medical applications)\n",
        "- **Generalization**: Performance on unseen data (from learning curves)\n",
        "\n",
        "### Recommendations:\n",
        "Based on the results, the recommended model should balance accuracy with interpretability, as medical professionals need to understand and trust the diagnostic system. The learning curve analysis helps ensure the model generalizes well to new data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Hyperparameter Tuning\n",
        "\n",
        "We'll perform hyperparameter tuning for Random Forest and SVM using GridSearchCV to optimize their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Random Forest Tuning\n",
        "print(\"Tuning Random Forest...\")\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "rf_base = RandomForestClassifier(random_state=42)\n",
        "rf_grid = GridSearchCV(rf_base, rf_param_grid, cv=5, scoring='accuracy', \n",
        "                      n_jobs=-1, verbose=1)\n",
        "rf_grid.fit(X_train.values, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters: {rf_grid.best_params_}\")\n",
        "print(f\"Best CV score: {rf_grid.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate tuned model\n",
        "rf_tuned = rf_grid.best_estimator_\n",
        "rf_tuned_pred = rf_tuned.predict(X_test.values)\n",
        "rf_tuned_acc = accuracy_score(y_test, rf_tuned_pred)\n",
        "print(f\"Test accuracy: {rf_tuned_acc:.4f}\")\n",
        "\n",
        "# Update model and results\n",
        "trained_models['Random Forest'] = rf_tuned\n",
        "rf_tuned_proba = rf_tuned.predict_proba(X_test.values)[:, 1]\n",
        "results['Random Forest'] = {\n",
        "    'accuracy': rf_tuned_acc,\n",
        "    'precision': precision_score(y_test, rf_tuned_pred, average='weighted', zero_division=0),\n",
        "    'recall': recall_score(y_test, rf_tuned_pred, average='weighted', zero_division=0),\n",
        "    'f1_score': f1_score(y_test, rf_tuned_pred, average='weighted', zero_division=0),\n",
        "    'auc_roc': roc_auc_score(y_test, rf_tuned_proba),\n",
        "    'y_pred': rf_tuned_pred,\n",
        "    'y_pred_proba': rf_tuned_proba\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. SVM Tuning\n",
        "print(\"\\nTuning SVM...\")\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1],\n",
        "    'kernel': ['rbf', 'poly']\n",
        "}\n",
        "\n",
        "svm_base = SVC(random_state=42, probability=True)\n",
        "svm_grid = GridSearchCV(svm_base, svm_param_grid, cv=5, scoring='accuracy',\n",
        "                       n_jobs=-1, verbose=1)\n",
        "svm_grid.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(f\"\\nBest parameters: {svm_grid.best_params_}\")\n",
        "print(f\"Best CV score: {svm_grid.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate tuned model\n",
        "svm_tuned = svm_grid.best_estimator_\n",
        "svm_tuned_pred = svm_tuned.predict(X_test_scaled)\n",
        "svm_tuned_acc = accuracy_score(y_test, svm_tuned_pred)\n",
        "print(f\"Test accuracy: {svm_tuned_acc:.4f}\")\n",
        "\n",
        "# Update model and results\n",
        "trained_models['SVM'] = svm_tuned\n",
        "svm_tuned_proba = svm_tuned.predict_proba(X_test_scaled)[:, 1]\n",
        "results['SVM'] = {\n",
        "    'accuracy': svm_tuned_acc,\n",
        "    'precision': precision_score(y_test, svm_tuned_pred, average='weighted', zero_division=0),\n",
        "    'recall': recall_score(y_test, svm_tuned_pred, average='weighted', zero_division=0),\n",
        "    'f1_score': f1_score(y_test, svm_tuned_pred, average='weighted', zero_division=0),\n",
        "    'auc_roc': roc_auc_score(y_test, svm_tuned_proba),\n",
        "    'y_pred': svm_tuned_pred,\n",
        "    'y_pred_proba': svm_tuned_proba\n",
        "}\n",
        "\n",
        "print(\"\\nâœ“ Hyperparameter tuning completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Evaluation and Comparison\n",
        "\n",
        "Let's create a comprehensive comparison of all models and identify the best performing one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Learning Curves\n",
        "\n",
        "Learning curves help us understand if a model is overfitting or underfitting by showing how the model's performance changes as the training set size increases.\n",
        "\n",
        "### Learning Curve for Best Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select best model for learning curve\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model = trained_models[best_model_name]\n",
        "\n",
        "# Determine which data to use\n",
        "if best_model_name in ['SVM', 'ANN', 'KNN']:\n",
        "    X_use = X_train_scaled\n",
        "else:\n",
        "    X_use = X_train.values\n",
        "\n",
        "print(f\"Generating learning curve for {best_model_name}...\")\n",
        "\n",
        "# Calculate learning curve\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    best_model, X_use, y_train, cv=5, \n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    scoring='accuracy', n_jobs=-1\n",
        ")\n",
        "\n",
        "# Calculate mean and std\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "val_mean = np.mean(val_scores, axis=1)\n",
        "val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Score', linewidth=2)\n",
        "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
        "plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation Score', linewidth=2)\n",
        "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
        "\n",
        "plt.xlabel('Training Set Size', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Accuracy Score', fontsize=12, fontweight='bold')\n",
        "plt.title(f'Learning Curve - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='best', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../Results/learning_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"âœ“ Learning curve saved\")\n",
        "\n",
        "# Interpretation\n",
        "print(\"\\nLearning Curve Interpretation:\")\n",
        "if train_mean[-1] > val_mean[-1] + 0.1:\n",
        "    print(\"  - Model shows signs of overfitting (training score much higher than validation)\")\n",
        "elif val_mean[-1] < 0.7:\n",
        "    print(\"  - Model shows signs of underfitting (both scores are low)\")\n",
        "else:\n",
        "    print(\"  - Model shows good generalization (training and validation scores are close)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "for name, metrics in results.items():\n",
        "    comparison_data.append({\n",
        "        'Model': name,\n",
        "        'Accuracy': metrics['accuracy'],\n",
        "        'Precision': metrics['precision'],\n",
        "        'Recall': metrics['recall'],\n",
        "        'F1-Score': metrics['f1_score'],\n",
        "        'AUC-ROC': metrics.get('auc_roc', np.nan)\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(\"Model Comparison Summary:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Identify best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
        "print(f\"   Accuracy: {comparison_df.iloc[0]['Accuracy']:.4f}\")\n",
        "print(f\"   F1-Score: {comparison_df.iloc[0]['F1-Score']:.4f}\")\n",
        "\n",
        "# Save comparison to CSV\n",
        "comparison_df.to_csv('../Results/model_comparison.csv', index=False)\n",
        "print(\"\\nâœ“ Comparison results saved to CSV\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualizations\n",
        "\n",
        "Let's create comprehensive visualizations to compare model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curves\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for name, metrics in results.items():\n",
        "    if metrics.get('y_pred_proba') is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, metrics['y_pred_proba'])\n",
        "        auc = metrics.get('auc_roc', 0)\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.3f})\", linewidth=2)\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
        "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../Results/roc_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"âœ“ ROC curves saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrices\n",
        "n_models = len(trained_models)\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (name, metrics) in enumerate(results.items()):\n",
        "    cm = confusion_matrix(y_test, metrics['y_pred'])\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    \n",
        "    # Plot normalized confusion matrix\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "               ax=axes[idx], cbar_kws={'shrink': 0.8})\n",
        "    axes[idx].set_title(f'{name}\\nAccuracy: {metrics[\"accuracy\"]:.3f}', \n",
        "                      fontweight='bold')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "    axes[idx].set_xticklabels(['No Disease', 'Disease'])\n",
        "    axes[idx].set_yticklabels(['No Disease', 'Disease'])\n",
        "\n",
        "# Hide unused subplot\n",
        "if n_models < len(axes):\n",
        "    axes[-1].axis('off')\n",
        "\n",
        "plt.suptitle('Confusion Matrices - All Models', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../Results/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"âœ“ Confusion matrices saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Comparison Bar Charts\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(comparison_df)))\n",
        "\n",
        "for idx, metric in enumerate(metrics_to_plot):\n",
        "    ax = axes[idx // 2, idx % 2]\n",
        "    bars = ax.barh(comparison_df['Model'], comparison_df[metric], \n",
        "                  color=colors, alpha=0.7, edgecolor='black')\n",
        "    ax.set_xlabel(metric, fontsize=11, fontweight='bold')\n",
        "    ax.set_title(f'Model Comparison - {metric}', fontsize=12, fontweight='bold')\n",
        "    ax.set_xlim([0, 1])\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "    \n",
        "    # Add value labels\n",
        "    for i, (bar, val) in enumerate(zip(bars, comparison_df[metric])):\n",
        "        ax.text(val + 0.01, i, f'{val:.3f}', va='center', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../Results/model_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"âœ“ Model comparison charts saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance for Tree-based Models\n",
        "tree_models = ['Decision Tree', 'Random Forest']\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "for idx, name in enumerate(tree_models):\n",
        "    if name in trained_models:\n",
        "        model = trained_models[name]\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importances = model.feature_importances_\n",
        "            feature_names = X_train.columns\n",
        "            \n",
        "            # Sort by importance\n",
        "            indices = np.argsort(importances)[::-1]\n",
        "            \n",
        "            ax = axes[idx]\n",
        "            ax.barh(range(len(importances)), importances[indices], \n",
        "                   color='steelblue', alpha=0.7, edgecolor='black')\n",
        "            ax.set_yticks(range(len(importances)))\n",
        "            ax.set_yticklabels([feature_names[i] for i in indices])\n",
        "            ax.set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
        "            ax.set_title(f'{name} - Feature Importance', fontsize=12, fontweight='bold')\n",
        "            ax.invert_yaxis()\n",
        "            ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Feature Importance Analysis', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../Results/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"âœ“ Feature importance plots saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusion and Recommendations\n",
        "\n",
        "Based on the comprehensive evaluation, we can now make data-driven recommendations for the best model for heart disease prediction in this context.\n",
        "\n",
        "### Key Findings:\n",
        "1. All models were successfully trained and evaluated\n",
        "2. Hyperparameter tuning improved performance for Random Forest and SVM\n",
        "3. The best model achieved high accuracy and F1-score\n",
        "\n",
        "### Model Selection Justification:\n",
        "The best model selection should consider:\n",
        "- **Accuracy**: Overall correctness of predictions\n",
        "- **Precision**: Minimizing false positives (critical in medical diagnosis)\n",
        "- **Recall**: Minimizing false negatives (missing actual disease cases)\n",
        "- **F1-Score**: Balanced measure of precision and recall\n",
        "- **AUC-ROC**: Overall discriminative ability\n",
        "- **Interpretability**: Ability to explain predictions (important for medical applications)\n",
        "\n",
        "### Recommendations:\n",
        "Based on the results, the recommended model should balance accuracy with interpretability, as medical professionals need to understand and trust the diagnostic system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Radar Chart Comparison\n",
        "\n",
        "A radar chart provides a comprehensive view of model performance across multiple metrics simultaneously.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create radar/spider chart for multi-metric comparison\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "num_metrics = len(metrics)\n",
        "\n",
        "# Calculate angles for each metric\n",
        "angles = np.linspace(0, 2 * np.pi, num_metrics, endpoint=False).tolist()\n",
        "angles += angles[:1]  # Complete the circle\n",
        "\n",
        "# Create figure\n",
        "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
        "\n",
        "# Plot each model\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(comparison_df)))\n",
        "\n",
        "for idx, row in comparison_df.iterrows():\n",
        "    values = [row[metric] for metric in metrics]\n",
        "    values += values[:1]  # Complete the circle\n",
        "    \n",
        "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[idx])\n",
        "    ax.fill(angles, values, alpha=0.15, color=colors[idx])\n",
        "\n",
        "# Customize\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(metrics, fontsize=11, fontweight='bold')\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], fontsize=9)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.title('Model Performance Radar Chart', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../Results/radar_chart.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"âœ“ Radar chart saved\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
